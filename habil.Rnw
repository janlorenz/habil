\documentclass[a5paper,10pt, twoside]{report}
\usepackage[top=11mm, inner=17mm, outer=11mm, bottom=17mm]{geometry}
\usepackage[table]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts, amsmath, amssymb, eurosym}
\usepackage{authblk}
\usepackage{paralist} 
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage[margin=5pt,font={footnotesize},labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{palatino}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beramono}
\usepackage{titlesec, blindtext}
\definecolor{gray75}{gray}{0.75}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter}{15pt}{\Huge\bfseries}
\usepackage[bookmarks=true,linkcolor=blue!80!black,
 citecolor=blue!80!black,urlcolor=blue!80!black,colorlinks=true,
 breaklinks=true, bookmarksopen=true]{hyperref}
\usepackage[backend=biber, hyperref=auto, style=alphabetic, citestyle=authoryear ]{biblatex}
\addbibresource{../literature/lit_lorenz.bib}
\defbibenvironment{bibliography}
  {\list{}
  {\setlength{\leftmargin}{\bibhang}%
  \setlength{\itemindent}{-\leftmargin}%
  \setlength{\itemsep}{\bibitemsep}%
  \setlength{\parsep}{\bibparsep}}}
  {\endlist}
  {\item}
\ExecuteBibliographyOptions{isbn=false,doi=false,url=false,firstinits=false,maxbibnames=99,maxcitenames=99,eprint=false}
\newbibmacro{string+doi+url}[1]{%
  \iffieldundef{doi}{\iffieldundef{url}{\iffieldequalstr{eprinttype}{jstor}{\href{http://www.jstor.org/stable/\thefield{eprint}}{#1}}{\iffieldequalstr{eprinttype}{arxiv}{\href{http://www.arxiv.org/abs/\thefield{eprint}}{#1}}{\iffieldequalstr{eprinttype}{googlebooks}{\href{http://books.google.com/books?id=\thefield{eprint}}{#1}}{#1}}}}{\href{\thefield{url}}{#1}}}{\href{http://dx.doi.org/\thefield{doi}}{#1}}}
\DeclareFieldFormat{title}{\usebibmacro{string+doi+url}{\mkbibemph{#1}}}
\DeclareFieldFormat[article]{title}{\usebibmacro{string+doi+url}{#1}}
\DeclareFieldFormat[incollection]{title}{\usebibmacro{string+doi+url}{#1}}
\renewbibmacro*{volume+number+eid}{%
  \printfield{volume}%
  \setunit*{\addthinspace}% NEW (optional); there's also \addnbthinspace
  \printfield{number}%
  \setunit{\addcomma\space}%
  \printfield{eid}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat{pages}{#1}
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\author{Jan Lorenz}
\affil{Jacobs University Bremen, Campus Ring 1, 28759 Bremen, post@janlo.de}
\date{\today}


\renewcommand{\baselinestretch}{1.1}
\begin{document}

<<R-setup, echo=FALSE, warn=FALSE>>=
## Packages
suppressMessages(library(tidyverse, warn.conflicts = FALSE, quietly = TRUE))
suppressPackageStartupMessages(library(reshape2, warn.conflicts = FALSE))
suppressPackageStartupMessages(library(fitdistrplus))
library(grid)
library(RColorBrewer)

# Scripts for certain topics
source("R/informationaccumulationsystem.r")
source("R/Ratings.r")
source("R/bc_markovchain.r")
source("R/woc.r")
source("R/wiserthancrowd.r")
# Load Data
G <- c(read.csv("R/galton_data.csv",header = FALSE)[[1]]) # Galton-Data
truth_G <- 1198
D <- tbl_df(read.csv("R/Viertelfest.csv")) %>% mutate(Estimate=Sch채tzung)
sD <- D$Estimate # Viertelfest-Data
rm(D)
truth <- 10788
expert <- 19100
O <- read.csv("R/ol_data.csv")
invalid_O <- sum(is.na(O$Frage1))
sO <- O[!is.na(O$Frage1),"Frage1"] # Oldenburg-Data
truth_O <- nrow(O)
rm(O)
@


\begin{titlepage}
\begin{center}
{\Huge \bf Systemic Effects in \\ Models of \\
Opinion Dynamics, \\ Societal Growth, \\ and the Wisdom of Crowds \\}
\vspace{7mm}
\Large
Cumulative Habilitation Work \\ 
for obtaining the academic degree \\[2mm] 
\emph{Dr. habil.} and the \emph{Venia Legendi} \\[2mm]
in partial fulfillment of the requirements of the \\ 
\emph{Policies for Habilitation Candidates} of \\[2mm]
\emph{Jacobs University Bremen} \\[2mm]
in the area of expertise \\[2mm]
\emph{Computational Social Sciences} \\[2mm]
submitted by \\[4mm]
\emph{\bf Dr. rer. nat. Jan Lorenz}\\[4mm]
Bremen, \today\\[7mm]
\end{center}
\end{titlepage}

% Suggestions for members of the Habilitation Committee \\ 
% (3-6 Jacobs faculty members and the dean): \\
% \begin{compactenum}
% \item Prof. Dr. Arvid Kappas (as dean)
% \item Prof. Dr. Klaus Boehnke
% \item Prof. Dr. Adalbert F. X. Wilhelm
% \item Prof. Dr. Dierk Schleicher 
% \item Prof. Dr. Adele Diederich
% \item Prof. Dr. Hildegard Meyer-Ortmanns
% \item Dr. rer. nat. habil. Jens Christian Claussen
% \end{compactenum}
% \mbox{}\\[1mm]
% 
% \noindent Suggestions for external referees \\
% (3 referees needed, 2 must be external): \\
% \begin{compactenum}
% \item Prof. Dr. Andreas Flache (Sociology, Uni Groningen)
% \item Prof. Dr. Rainer Hegselmann (Philosophy and Social Simulation, Uni Bayreuth)
% \item Prof. Dr. Susumu Shikano (Political Science, Uni Konstanz)
% \end{compactenum}

\newpage

\mbox{}\thispagestyle{empty}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% PREFACE %%%%%%%%%%%%%%%%%
\section*{Preface}

 This habilitation work is an outcome of my academic journey from mathematics over the physics of socio-economic systems to the social sciences.
 This journey started with the fascination of mathematical models which are just a little bit too complicated to understand them analytically upfront and the use of computer simulation. It was further stimulated by the search for universal laws of human behavior, a bias for counterintuitive explanations, and the ambition to find and quantify hidden societal driving forces behind social phenomena. 
 Finally, the wish to gain insights which are directly meaningful in the social and economic world has driven me towards data analysis, data-driven modeling, and data production through experiments. 

 This booklet presents some of my contributions to a deeper understanding of \emph{opinion dynamics}, \emph{societal growth}, and the \emph{wisdom of the crowd}. 
 It shall show my ability to further pursue research and teaching in \emph{computational social science}. 
 Most of the theories and results in this booklet are taken and summarized from twelve independent scientific works which I authored. In many cases together with others. 
 Nine works have been published in peer-reviewed journals spanning political science, sociology, psychology, economics, and physics as well as two multi-disciplinary journals.   
 One publication appeared in an edited scientific year book and two as contributions to conferences in political science and social simulation. 

The works ordered by type and year of publication are:
\paragraph{Journal Publications}
\begin{compactenum}
\item \fullcite{Lorenz2009Universalityofmovie}
\item \fullcite{Shin.Lorenz2010TippingDiffusivityin}
\item \fullcite{Rauhut.Lorenz2010wisdomofcrowds}
\item \fullcite{Lorenz.Rauhut.ea2011HowSocialInfluence} \\
Reply Letter: \fullcite{Rauhut.Lorenz.ea2011ReplytoFarrell}
\item \fullcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth}
\item \fullcite{Lorenz.Rauhut.ea2015MajoritarianDemocracyUndermines}
\item \fullcite{Groeber.Lorenz.ea2014Dissonanceminimizationas}
\item \fullcite{Koenig.Lorenz.ea2016Innovationvsimitation}
\item \fullcite{Lorenz.Paetzel.ea2016JustDontCall}
\end{compactenum}

\paragraph{Book Chapters and Conference Papers}
\begin{compactenum}
\setcounter{enumi}{9}
\item \fullcite{Lorenz2012ZurMethodeder}
\item \fullcite{Metz.Lorenz2013Becomewhoyou}
\item \fullcite{Lorenz2015ModelingEvolutionIdeological}
\end{compactenum}

\bigskip 

 This booklet serves as an umbrella text for these papers but it also includes some new works. Section \ref{sec:opd} (comparison of opinion pattern diagrams), and Sections \ref{sec:guesstimation}, \ref{sec:quantwoc}, and \ref{sec:wocnew} (analysis of three guesstimation game datasets, a thorough definition of crowd wisdom, and results on the existence of experts and the power of diversity sampling in guesstimation) are new and not published.  
 
 \medskip

\hfill \emph{Jan Lorenz}, Bremen, December 2016



\tableofcontents



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% CHAPTER 1 %%%%%%%%%%%%%%%%%
\chapter{Introduction}\mbox{}\\[-1cm]
{\footnotesize
Papers of the Habilitation Work: \\[-1.5mm]
\begin{compactenum}
\item \fullcite{Lorenz2012ZurMethodeder}
\item \fullcite{Groeber.Lorenz.ea2014Dissonanceminimizationas}
\end{compactenum}
}
\vspace{1cm}

Socio-economic dynamics are created by the interaction of individuals but their emergent consequences often go beyond the scope of these individuals. 
 Dynamics of attitudes and opinions, the exchange, aggregation, and redistribution of resources and knowledge can lead to interesting systemic effects such as societal consensus or emerging bipolarization, economic growth or decline, crowd wisdom or crowd madness. Such effects are the subject of this book.
 The understanding of systemic effects in socio-economic systems is crucial for anticipation of potential societal change and the proper design of societal institutions. 

To that end let us define \emph{systemic effects} as system-wide effects which are triggered through local interaction. 
That means, systemic effects in a system can not be boiled down to a simple aggregate of properties of its units but only to more complex ones. 

Studying systemic effects requires a well-suited description of the units of a system and their action and interaction. 
For socio-economic systems that means the development of an \emph{agent-based model} with humans (or organisations) as the agents. 
Models are called agent-based when the actions and interactions of agents are explicitly part of the model. In that sense, agent-based models differ from variable-based models where relations are modelled between different variables, e.g. by structural equation modeling. 
Agent-based models specify how agents change the values of their variables as a result of interaction. 
Agent-based models are often studied with \emph{computer simulation}. 
How this method can be used to tackle political science research questions is further discussed and explained in  \textcite{Lorenz2012ZurMethodeder}.

The actions and interactions of agents in the model are defined through behavioral rules which are usually modeled in a simple direct way and not necessarily based on utility maximization. This distinguishes agent-based models from game-theoretic ones which are based on rational choice decision making. 
Agents in typical agent-based models apply these rules repeatedly in response to the changing societal context they observe. 
The idea of rational-choice models instead is that rational actors anticipate the rational moves of others which implies that all immediately switch to rational play such that their actions form a game-theoretic equilibrium. In game-theory, the existence and uniqueness of equilibrium solutions is of first interest and not so much the way how an equilibrium is reached or which of many equilibriums will be reached. 

Some agent-based modelers therefore emphasize that they are able to study systems dynamics ``out of equilibrium''.  
Although the dynamical perspective is indeed lacking to a large extent in rational choice theories, the oppositional view of agent-based and rational choice models is mostly overstated.
First, every dynamical model -- be it with complex or simple behavioral rules -- has the tendency to approach attractive fixed points (equilibria), either static, stochastic, cyclic, or chaotic. Thus, the notion ``out-of-equilibrium'' transfers either to the study of transition, or a restricted definition of equilibrium, or a not completely specified model. 
Second, behavioral rules can often be derived from properly designed utility functions which agents maximize or satisfice. The utility maximizing and the behavioral rules framework are thus not necessary contrary but can even be identical. Game-theoretic reasoning can also be fed into a dynamical agent-based model through an evolutionary or boundedly rational setting or incomplete information. For example a model where agents act rational but only with respect to information from a smaller subset of players and anticipate only with a limited time horizon. Under such assumptions, also rational-choice models give rise to systemic effects which go beyond the scope of the individual. 

\textcite{Groeber.Lorenz.ea2014Dissonanceminimizationas} (another part of the habilitation work) show an example how behavioral rules in models of continuous opinion dynamics can be microfounded as short-sighted (or myopic) best response of an individual to minimize opinion dissonance within its social context. In particular, the paper shows for which dissonance functions. Dissonance minimization can serve as a microfoundation of averaging as the behavioral rule in opinion dynamics models. In that sense, agents in many opinion dynamics models appear as boundedly rational agents which links agent-based modeling back to decision and game theory. 
\smallskip 

Agent-based modeling is either theory-driven or data-driven.
\emph{Theory-driven modeling} starts with a plausible setup of variables, parameters and microscopic behavioral rules derived from some theory or evidence from the micro-level.
\emph{Data-driven modeling} starts with the exploration of observation data by the extraction of interesting  stylized macroscopic facts. 
Either way, the idea is to explain macroscopic phenomena through human interaction on the microscopic level. 
Also controlled \emph{group experiments} can elicit systemic effects. 

Table \ref{tab:habilworks} lists ten publications of the habilitation work which will be part of the following three chapters grouped by theory-driven modeling, data-driven modeling, and experimental methods.  

\begin{table}[htbp]
\footnotesize
\def\tabularxcolumn#1{m{#1}}
\begin{tabularx}{\textwidth}{@{}l *3{>{\raggedright\arraybackslash}X}@{}} \toprule
                 & \multicolumn{3}{c}{Systemic effects studied} \\ \cmidrule{2-4}
Topic            & theoretically & based on data & experimentally \\ \midrule\midrule
                 &               & \textcite{Lorenz2009Universalityofmovie} &  \\   
Opinion Dynamics & \textcite{Shin.Lorenz2010TippingDiffusivityin} & \textcite{Metz.Lorenz2013Becomewhoyou} & \\
                 &               & \textcite{Lorenz2015ModelingEvolutionIdeological} & \\  \midrule
Societal Growth  & \textcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth} & \textcite{Koenig.Lorenz.ea2016Innovationvsimitation} & \textcite{Lorenz.Paetzel.ea2016JustDontCall} \\ \midrule
                 & & & \textcite{Rauhut.Lorenz2010wisdomofcrowds} \\
Wisdom of Crowds & & & \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} \\ 
                 & & & \textcite{Lorenz.Rauhut.ea2015MajoritarianDemocracyUndermines} \\ \bottomrule
\end{tabularx}
\caption{Ten habilitation works of Chapters 2, 3, and 4 grouped by methodological focus and topic.} \label{tab:habilworks}
\end{table}

 Chapter \ref{ch:od} outlines four models of opinion dynamics which include four different mechanisms which can produce bipolarized opinion landscapes. 
 The relation of bipolarization and the core parameters of these models are shown and compared through opinion pattern diagrams in a final section. 
 Chapter \ref{ch:growth} shows two models of growth and one experiment about a strong framing effect. It concludes with a discussion on systemic growth effects. 
 Chapter \ref{ch:woc} presents the wisdom of crowd phenomenon in guesstimation games as a systemic property of samples and experimental results how it changes through social influence and institutions for collective decision are presented. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% CHAPTER 2 %%%%%%%%%%%%%%%%%
\chapter{Opinion Dynamics}\label{ch:od}
\mbox{}\\[-1cm]
{\footnotesize
Papers of the Habilitation Work: \\[-1.5mm]
\begin{compactenum}
\setcounter{enumi}{2}
\item \fullcite{Shin.Lorenz2010TippingDiffusivityin}
\item \fullcite{Metz.Lorenz2013Becomewhoyou}
\item \fullcite{Lorenz2009Universalityofmovie}
\item \fullcite{Lorenz2015ModelingEvolutionIdeological}
\end{compactenum}
}
\vspace{1cm}

\hfill\parbox{100mm}{
\footnotesize
Since universal ultimate agreement is an ubiquitous outcome of a very broad class of mathematical models, we are naturally led to inquire what on earth one must assume in order to generate the bimodal outcome of community cleavage studies. \hfill Robert P. Abelson (\citeyear{Abelson1964Mathematicalmodelsof})
}

\medskip

When individuals form opinions on certain topics this happens on the one hand based on individual knowledge, experience, and emotions. On the other hand, opinions are to a large extent intrinsically social. 
We as citizens, employees, consumers, and in other roles adjust our existing opinions once we hear the opinions of others because we learn new information, because we want to conform with, or confront the opinions of others. Sometimes we consciously form our opinions entirely based on how it best fits in the opinion landscapes of our social environment (see \textcite{Groeber.Lorenz.ea2014Dissonanceminimizationas}). 

How individuals adjust their opinions with respect to messages from others has been studied in \emph{social psychology} in the field of \emph{attitude research}.
The focus of \emph{opinion dynamics} goes beyond the individual and extends the view from individual processes to systemic processes triggered by them. How do repeated opinion formation and adjustment shape the entire opinion landscape to phenomena like consensus, bipolarization, or opinion plurality? 

The following four sections present four models of opinion dynamics, which include four different mechanisms how societal bipolarization maintains itself or even evolves. These mechanisms are compared with opinion pattern diagrams in a final section. 


%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 2, TOPIC 1, Tipping Diffusivity %%%%%%%%%%%%%%%%%%%
\section{Tipping diffusivity and why more links can lead to less consensus}  \label{sec:OD1}

 \textcite{Shin.Lorenz2010TippingDiffusivityin} model a situation of two equally sized communities of agents which both are characterized by a typical number of intra-community links between agents of the same community and a typically lower number of inter-community links of members from different communities.  
 All agents hold an opinion which is a real numbers between -1 and +1. It might represent a tendency towards different opposing cultures, while zero represents no opinion on this. 
 We further assume that initially all agents of each community hold the same opinion, while 
one community has a negative one, and the other a positive one. 
 ``Negative'' and ``positive'' are used in the mathematical sense here, abstracting from any value judgment. 
 Our theoretical question is under what conditions can these communities maintain their different opinions. 

Opinion dynamics is modeled as an information accumulation system \parencite{Shin2009Informationaccumulationsystem}. ``Information'' here means opinion towards one of the two opposites -1 or +1. In each iteration agents on the one hand forget part of that information (which would imply convergence to the ground state zero) and accumulate on the other hand opinions of the agents within their network which includes many agents from the same community and few agents from the other community. We assume that all agents in one community start with the same opinion and that all have the same amount of intra- and inter-community links. Therefore, the opinions of all agents in one community opinions always move synchronously. As a consequence, we can study just two coupled dynamical equations for  $y_1$ and $y_2$ representing the opinions in each community. 
The global variables of the model are: the \emph{intra-community diffusivity} $\Omega_0$, the \emph{inter-community diffusivity} $\Omega_X$, and the \emph{rate of forgetting} $\Delta$. ``Diffusivity'' is a notion which can simply be derived from the number of intra- and inter-community links. The dynamical equations and their derivation from the agent-based model are shown in \textcite{Shin.Lorenz2010TippingDiffusivityin}. 

The dynamics of this system can be completely characterized, including all fixed points, their status of attractivity and stability, and their basins of attraction \parencite[see][]{Shin.Lorenz2010TippingDiffusivityin}. Figure \ref{fig:ShinLorenz} shows a phase diagram with respect to intra- and inter-community diffusivities. It shows which fixed points of which kind exist in which regions of the $(\Omega_0,\Omega_X)$-phase space.

\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figs/fig2_3_blog-crop.pdf}
\caption{Phase diagram of intra- vs. inter-community diffusivity $(\Omega_0,\Omega_X)$ parameterized in units of the rate of forgetting $\Delta$ (top panel), with five exemplary $(y_1,y_2)$-planes which show the fixed points for the opinions of the two communities and their basins of attraction for different initial opinions (central panel), and with ten exemplary trajectories over time for different initial opinions (peripheral panels). } \label{fig:ShinLorenz}
\end{figure}

The most interesting prediction of the model is summarized in the title of the paper: ``More links, less consensus''. This result can be best understood by following the red arrow in Figure \ref{fig:ShinLorenz}. This arrow represents situations where there is always four times more intra-community diffusivity than inter-community diffusivity but along the arrow the total diffusivity (equivalently the number of links) increases. The phase diagram is parameterized in units of the rate of forgetting. The ``journey'' along the arrow goes as follows:
\begin{compactenum}[(1)]
 \item Initially we are in the trivial zone. The rate of forgetting dominates and trajectories converge to the ground state or ``no opinion'' for any initial opinions. 
 \item We cross the border to the consensus zone. Now two fixed points emerge, and both communities converge to a either a positive or a negative opinion. The system converges to the opinion of the community starting with the more extreme opinion. It is still possible for the system to converge to the ground state but only for a thin set of initial values (shown as a white diagonal line). This is a typical example of an unstable but attractive fixed point. 
 \item We cross the dotted line and are still in the consensus zone, but two unstable fixed points with coexisting different opinions in communities emerge. Even the smallest fluctuation in opinions around these fixed points of coexistence would bring the system to the basin of attraction of one of the two consensual fixed points. 
 \item We cross the border to the coexistence zone where both fixed points with coexistence become stable. The further we go the larger their basins of attraction grow. 
\end{compactenum}
One reason for the emergence of the coexistence zone is that the dynamical equation has a saturation component. Saturation prevents that agents accumulate information in one direction ad infinitum. Information reasonably saturates at either +1 or -1. 

A hypothesis which one can derive from this model is that coexistence of different cultures in connected communities is more likely when the overall connectivity is larger. We think that this might be part of an explanation for the increasing polarization and fragmentation of political debates which seems to happen nowadays through increasing  connectivity through social networks on the internet. The next step should be to bring this hypothesis to an empirical test. 

\bigskip

While the development of this model is mostly theory-driven, the next three models are data-driven on different types of opinions. Every description therefore starts with a brief description of the stylized facts observed in data. 

\bigskip


%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 2, PAPER 2, Political Partisanship %%%%%%%%%%%%%%%%%%%
\section{How political partisanship forms through self-reinforcement}  \label{sec:OD2}
The \textcite{GSPS2010datayears1984-2010} asked their participants if they feel attached to a certain party and to which. Despite of panel attrition and other exclusion criteria, an impressive number of 965 individuals remain which unambiguously name either no attachement or attachment to one of the four West-German political parties (CDU/CSU, SPD, FDP, Gr체ne) in every of the 27 years from 1984 to 2010. 
Figure \ref{fig:MetzLorenz} is a histogram of these 965 individuals showing how many individuals made which amounts of announcement for a left (SPD and Gr체ne) and right (CDU/CSU and FDP) party. In particular, the histogram shows, that there are about 100 people who made 27 announcements of attachments to a left party (leftmost bin). Also about 100 people announced 27 times an attachment to a right party (rightmost bin). About 40 people never announced any party attachment (most backward bin). People announcing an attachment to a left (right) party but not all the time exist in large magnitude. People who announce a large number announcements for left and right parties are rare. For example there is only one person which announced eleven times a left party and also eleven times a right party. 
The evolution ofsuch histograms is modeled in \textcite{Metz.Lorenz2013Becomewhoyou}. 

\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figs/3dPlotSideChoice_qti.pdf}
\caption{Histogram indicating the count of individuals announcing attachment with left (SPD, Gr체ne) and right (CDU/CSU, FDP) parties.} \label{fig:MetzLorenz}
\end{figure}

The main stylized facts of this data are: 
\begin{compactenum}[(i)]
 \item Party attachment is almost exclusively either to right or to left parties. The decision of an individual is thus mostly if the left (or right) attachement is announced or not and not which of the two sides is chosen. 
 \item The histogram (count of people) of the number of uttered attachment within a time interval (here 27 years) has a U-shape. That means, there are peaks at the extremes for no attachment ever (0 announcments) and always attached (27 announcments). Further on, there are more people close to these extremes than for an intermediate number of announcements.  
\end{compactenum}

The paper presents a stochastic model for the utterance of party attachments by individuals over time. 
Based on stylized fact (i) the left-right divide was neglected. The paper concentrates on the homing-in and homing-out process. The dynamic variable for each individual over the 27 years is thus if she or he utters an attachment or not. The U-shape leads to the assumption of some self-reinforcing for utterance and non-utterance over time. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figs/EvolutionOfMarkovChain.pdf}
\caption{Evolution the interactive Markov chain for the probability distribution of the numbers of attachments in 0, 1, 2, ..., 10 years.}\label{fig:MetzLorenz2}
\end{figure}

The model idea is the following: 
We focus on the individual and its memory of past announcements of party attachments. 
In year one an individual announces a party attachment with probability $0 \leq q \leq 1$. 
In year two this individual announces a party attachment with probability $\frac{q}{2}$ if it had not announced an attachment before and with probability $\frac{q+1}{2}$ if it had announced one. 
The probability of an announcement is thus more likely if an announcement happened before. 

For a generic mechanism we define $x(t) \in \{0,1\}$ to be the \emph{announcement of the individual} at time step $t$, $\bar{x}(t) = \sum_{s=1}^t x(t)$ as the \emph{number of announcements} up to $t$, and $q$ as the \emph{interest in politics}. 
We define the \emph{probability to announce a partisanship} at time $t$ as $p(t,\bar{x}(t),q) = \frac{\bar{x}(t) + q}{t+1}$. 
Consequently $x(t+1)=1$ with probabilty $p(t,\bar{x}(t),q)$ and $x(t+1) = 0$ otherwise.

The stochastic process $(x(t))_{t}$ is thus not a Markov process as the transition probabilities depend not only on the latest state but on all past states. 
The analog process for the number of announcements so far $(\bar{x}(t))_t$ instead is a time-dependent Markov process, because $\bar{x}(t+1) = \bar{x}(t) + 1$ with probability $p(t,\bar{x}(t),q)$ and $\bar{x}(t+1) = \bar{x}(t)$ otherwise. Here, the transition probabilities only depend on the current state and the time step. 
This evolution of the probability distribution of the number of announcements $\bar{x}(t)$ can be easily computed by iteratively multiplying the time-dependent transition matrices of the Markov process. 
Figure \ref{fig:MetzLorenz2} shows how the probability distribution evolves for $q=0.641$. 

 Theoretically, it turns out the distribution of the number of announcements after $t$ time steps is Beta-binomial distribution for $t$ trials with $\alpha = q$ and $\beta = 1-q$. 
 Another way to see the model is as a kind of urn model: 
 Each announcement is a random draw from the urn where all past announcements and non-announcements are represented by balls with two different colors plus one additional special ball which shows the announcements color with probability $q$ and the non-announcement color with probability $1-q$. 

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{figs/ParameterFit.pdf}
\caption{Fit between SOEP data and theory (q = 0.641) for number of attachments. 
Data from Markov chain, confidence intervals from 1000 runs of agent-based simulation with 965 agents. 
Insets: Model behavior for first 10 and first 20 steps of the panel.}
\label{fig:MetzLorenz3}
\end{figure}

 The parameter $q$ was fitted to best match the data. The best fit was achieved for $q=0.641$. 
 The model fit is shown in Figure \ref{fig:MetzLorenz3}. 
 The model fit has a striking coefficient of determination of $R^2 = 0.958$. 
 This shows that the idea to model announcements of party attachment as a self-re-inforcing stochastic process already explains the evolving U-shaped distribution quite well. 
 In particular, it explains the co-existence of large fractions non- and strong partisans with smaller fraction individuals homeing in and out with just one parameter -- the interest in politics $q$.

For smaller interest in politics $q<0.641$ the distribution would lead to a U-shape with a higher amount of never attached and a lower amount of always attached individuals. For higher interest in politics $q<0.641$ the bin with always attached individuals would be even higher and the bin with never attached individuals even lower. In the paper, it is further analysed if this would realize when we subdivide our sample based on the survey question about individuals' political interest in the \textcite{GSPS2010datayears1984-2010}. It turns out that fitting the model parameter $q$ for the different subsamples of people gives us $q=0.82$ for individuals with very strong political interest, $q=0.7$ for strong, $q=0.58$ for weak, and $q=0.41$ for no interest in politics with good model fits. 

The reinforcement of individual attachment and non-attachment announcments in the model drives the evolution of the U-shaped distribution leading to a partly ``polarized'' population into strong party supporters and people never supporting any party. Note, that this mechanism is not social as the iteraction is only with the own past. 
\bigskip


%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 2, TOPIC 3, Movie ratings %%%%%%%%%%%%%%%%%%%
\section{How movie rating histograms shape through averaging and over- and undershooting} \label{sec:OD3}

 Nowadays, $\bigstar$-based rating systems are ubiquitous. 
 Almost everything can be rated with some $\bigstar$s. 
 The costs of rating is just one click. 
 Movies gain several 10,000 or even 100,000 ratings on popular sites. 
 The ``Internet Movie Database'' (\texttt{\href{http://imdb.com}{IMDb.com}}) uses a 10$\bigstar$-scale. 
 Of most interest to users is the average number of $\bigstar$s, which is prominently displayed in the website.
 This number influences the decisions of others to watch the movie. 
 Further on, the user-movie-matrix of ratings is the basis for personalized recommendation systems. 
 Besides the average, also the histogram of the number of $\bigstar$s given by users is accessible. 
 \textcite{Lorenz2009Universalityofmovie} analyzed histograms of all movies with more than 20,000 ratings\footnote{These were 1,086 movies by March 2008. For an impression of the trend: By Sep 6, 2016 IMDb has 1,403 titles with more than 100,000 votes. This includes series now.}, and tries to identify universal characteristics which say something about underlying processes of continuous opinion dynamics. 
 10$\bigstar$-rating scales are a good approximation of continuous opinions because ratings are adjustable and fairly fine-grained. 

The main stylized facts (cf.~\ref{fig:IMDbexamples}) are:
\begin{compactenum}[(i)]
\item Histograms have {`Gaussian-like'} shape on the central bins (2$\bigstar$--9$\bigstar$). That means there is only one peak and the decay when deviating from the peak looks bell shaped. 
\item {Extreme} bins (1$\bigstar$ and $10\bigstar$) are higher than expected from extrapolating the bell shape from central bins.
\end{compactenum}
 Consequently, movie histograms have either three or two peaks. 
 When there are three peaks, two are at the extremes. 
 When there are two peaks one or both are at the extremes. 
 There are only 5 movies (0.5\%) exemptions\footnote{In 4 cases the 2$\bigstar$-bin is slightly higher than 3$\bigstar$-bin but less than the 1$\bigstar$-bin. 
 One movie has an almost invisible additional peak at the 3$\bigstar$-bin.}. 
 Nine examples are shown in Figure \ref{fig:IMDbexamples}.
 
\begin{figure}[htb]
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb2}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb3}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb4}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb5}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb6}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb7}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb8}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb9}
\includegraphics[width=0.32\textwidth]{/home/janlo/Documents/posters/2009_Rome/figs/margin/imdb10}
\caption{Nine movie rating histogram examples from IMDb together with a corresponding $\mu$-fitted confined Levy-skew $\alpha$-stable histogram (red lines).} \label{fig:IMDbexamples}
\end{figure}

Both stylized facts are explained through two assumptions on the process of opinion formation:
\begin{compactenum}[(a)]
\item A rating is the average of opinions of others, the current average rating, and several independent criteria. Thus, the distribution of ratings has a Gaussian-like shape due to the central limit theorem. \label{enum:rating1}
\item Ratings are {discretised and confined} continuous opinions. All probability mass which over- or undershoots beyond the rating scale has to be collected in the extremal bins. \label{enum:rating2}
\end{compactenum}
The idea of (\ref{enum:rating2}) is for example that individuals who want to assess a movie as 15$\bigstar$ or -1$\bigstar$ have to confine their estimate between 1$\bigstar$ and 10$\bigstar$. Regarding (\ref{enum:rating1}), the only distribution which evolves under averaging in the limit of many criteria is the \emph{L\'evy skew $\alpha$-stable distribution} $S(\alpha,\beta,\gamma,\mu)$. Thus, it is a natural candidate to characterize rating histograms. 
Its probability density function is
\begin{displaymath}
  P(x) = \frac{1}{2 \pi} \int_{-\infty}^{+\infty} \varphi (t)
  e^{-itx}\,dt, \quad \text{with } \varphi(t) = e^{\mu\!-\!|\gamma t|^\alpha\,(1\!-\!i\beta\,\textrm{sign}(t)\Phi)},
\end{displaymath}
with $\Phi = \tan(\frac{\pi\alpha}{2})$ if $\alpha\neq 1$ and $\Phi=-\frac{2}{\pi}\log |t|$ for $\alpha=1$.
Parameters characterize  peakedness $\alpha \in (0,2]$, skewness $\beta \in [-1,1]$, scale (or dispersion) $\gamma \in [0,\infty)$, and location $\mu \in (-\infty,\infty)$ of the distribution of opinions as shown in Figure \ref{fig:levy}.
The normal distribution comes out for $\alpha=2$, in this case $\beta$ has no effect. For $\alpha<2$ the distribution has power-law tails which decay on both sides as $P(x) \propto |x|^{-(1+\alpha)}$. The mean of the distribution exists for $\alpha>1$ and is $\mu$. 
 The generalized central limit theorem states that the average of several random variables which distributions have power law tails with the same exponent converge to a L\'evy skew $\alpha$-stable distribution. 
 From a continuous L\'evy skew $\alpha$-stable distribution, a \emph{confined $(\alpha,\beta,\gamma,\mu)$-histogram} of ratings can be produced by discretization as shown in Figure \ref{fig:discretization}.

\begin{figure}[htbp]
\includegraphics[width=0.32\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figExplainLevyA.pdf}
\includegraphics[width=0.32\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figExplainLevyB}
\includegraphics[width=0.32\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figExplainLevyC}\\
\tikz\draw (0,0) node{$\mu$} (4,0)node{$\mu$} (8,0)node{$\mu$};
\caption{How the parameters of the L\'evy-skew $\alpha$-stable distribution modify the shape of the distirbution compared to $S(\frac{4}{3},0,1,\mu)$, where $\mu$ is the mark on the real line. } \label{fig:levy}
\end{figure}

\begin{figure}[htbp]
\includegraphics[width=0.45\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figExplainDiscretisation1}
\begin{tikzpicture}
\path (0,0) -- (0,0.8cm )node {\Large $\leadsto$};
\end{tikzpicture}
\includegraphics[width=0.45\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figExplainDiscretisation2}
\caption{Discretization of continuous distribution with confinement. }\label{fig:discretization}
\end{figure}

 Confined $(\alpha,\beta,\gamma,\mu)$-histograms were fitted to the empirical IMDb-histograms \parencite{Lorenz2009Universalityofmovie}. 
 Figure \ref{fig:LevyParams} shows how the fitted $\alpha$, $\beta$ and $\gamma$ are distributed with respect to $\mu$. 
 The figure shows that the smallest scale parameter $\gamma$ as well as no skewness ($\beta=0$) appear both for $\mu\approx 7.5$. This $\mu$-value also coincides with the overall average $\mu$. 
 Thus, average movies tend to have the lowest dispersion of ratings with no skewness. 
 Further on, the peakedness $\alpha$ is relatively constant at around $\frac{4}{3}$ with respect to $\mu$.
 Skewness shows a linear trend with movies better than average being left-skew and movies worse than average being right skew. 
 That means, better movies even have a heavier tail in the very good ratings, while bad movies have a heavier tail in the bad ratings. 
 The scale parameter increases with a parabolic function when $\mu$ deviates from the average rating. 
 From these regularities -- linear $\beta$, quadratic $\gamma$, and constant $\alpha$ -- a one-parameter fit called {$\mu$-fit} was constructed and all movies where fitted again by only using the parameter $\mu$. The other three parameters of the confined $(\alpha,\beta,\gamma,\mu)$-histograms are just functions of $\mu$ corresponding to the green lines in Figure \ref{fig:LevyParams}.

\begin{figure}[htbp]
\includegraphics[width=0.32\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figLevyParams1}
\includegraphics[width=0.32\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figLevyParams2}
\includegraphics[width=0.32\columnwidth]{/home/janlo/Documents/posters/2009_Rome/figs/figLevyParams3}\\
\caption{Fitted Levy-skew $\alpha$-stable parameters of all 1,086 movies with respect to the location parameter $\mu$ (also fitted). Blue lines show averages between gridlines. Green lines show constant, linear and quadratic fits.}\label{fig:LevyParams}
\end{figure}

The $\mu$-fit in comparison to the original empirical histograms is shown for the nine example movies in Figure \ref{fig:IMDbexamples}. In summary, the quality of the one parameter fit is impressive. The average coefficient of determination is $R^2=0.943$.

 In conclusion: Ratings as discretized and confined averaged continuous opinions leads to very good approximations of real-world rating histograms. 
 The underlying {opinion distribution is not normal but fat-tailed}. 
 The one-parameter $\mu$-fit is still good and shows surprising regularities: an average movie ($\mu\approx 7.5$) has most narrow distribution and no skewness. 
 Deviation from an average movie makes the opinion distribution broader and skew pronouncing the deviation. 
 The surprising accuracy achieved with only one parameter points to some non-trivial regularities in the process of opinion formation for movie ratings.
Despite these indications of universality, the results can also be used to characterize movies in a new way by quantifying the deviations from theoretical histograms.
 This model also shows that confinement of over- and undershooting is a realistic model for larger extremal peaks. 

\bigskip


%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 2, TOPIC 4, Ideology Landscapes %%%%%%%%%%%%%%%%%%%
\section{How clusters in ideology space emerge through opinion dynamics under bounded confidence} \label{sec:OD4}

The bounded confidence model \parencite{Krause2000DiscreteNonlinearand, Hegselmann.Krause2002OpinionDynamicsand, Deffuant.Neau.ea2000MixingBeliefsamong, Weisbuch.Deffuant.ea2002Meetdiscussand} has been extensively studied theoretically and with computer simulation 
\parencite{Dittmer2001Consensusformationunder, 
Fortunato2004Universalityofthreshold,
Hegselmann.Krause2005OpinionDynamicsDriven,
Fortunato.Latora.ea2005VectorOpinionDynamics, 
Lorenz2006Consensusstrikesback,
Lorenz2006ContinuousOpinionDynamics,
Hegselmann.Krause2006TruthandCognitive,
Blondel.Hendrickx.ea20072Rconjecturemulti-agent, 
Lorenz2007RepeatedAveragingand, 
Lorenz2007ContinuousOpinionDynamics,
Urbig.Lorenz.ea2008Opiniondynamicseffect, 
Lorenz2008ManagingComplexityInsights, 
Pineda.Toral.ea2009Noisycontinuous-opiniondynamics, 
Lorenz2010Heterogeneousboundsof, 
Bertotti.Delitala2010Clusterformationin, 
Pineda.Toral.ea2011Diffusingopinionsin,
Nyczka2011ModelofOpinion, 
Mirtabatabaei.Bullo2012OpinionDynamicsin, 
GomezSerrano.Graham.ea2012BoundedConfidenceModel, 
Pineda.Toral.ea2013noisyHegselmannKrausea, 
Carro.Toral.ea2013RoleNoiseand, 
Pineda.Toral.ea2013noisyHegselmannKrause,
BenNaim.Scheel2015PatternSelectionand, 
Hegselmann.Koenig.ea2015OptimalOpinionControl, 
KurahashiNakamura.Maes.ea2016RobustClusteringin,
Mathias.Huet.ea2016BoundedConfidenceModel, 
Gargiulo.Gandic2016rolehomophilyin}. One reason for its popularity is the simplicity of its mechansism to produce cluster patterns. \textcite{Lorenz2015ModelingEvolutionIdeological} is a first attempt to apply the model ``as is'' to explain the shape of real-world opinion landscapes in the political and ideological realm. 

The data use by \textcite{Lorenz2015ModelingEvolutionIdeological} is from two items of the European Social Survey \parencite{ESS6}. People from a representative sample are asked for their ideological left---right selfplacement with the question ``In politics people sometimes talk of `left' and `right'. Using this card, where would you place yourself on this scale, where 0 means the left and 10 means the right?'' They are asked about European unification with the question ``Now thinking about the European Union, some say European unification should go further. Others say it has already gone too far. Using this card, what number on the scale best describes your position? [0=`Unification has already gone too far', 10=`Unification should go further']''. Figure \ref{fig:ess} shows histograms of answers over the samples in France, Germany, Sweden, and the United Kingdom using the design weights provided by ESS. These weights correct for different selection probabilities triggered by the sampling design. Let us call such histograms \emph{ideology landscapes} or \emph{opinion landscapes}. 

Opininon landscapes show the following stylized facts:
\begin{compactenum}[(i)]
 \item They never show the characteristics of a simple distribution, like the normal, uniform, or beta distribution. 
 \item The largest peak is always central.
 \item Multiple peaks are almost ubiquitous.
 \item Very often peaks at the extremes exist.
 \item Often there are intermediate off-center peaks e.g at 3 or 4 and 7 or 8.
\end{compactenum}

\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[ultra thick,red]
 \node at (0,0) {\includegraphics[trim=40 40 40 40,width=0.64\columnwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_lrscale_2012_FR}};
 \draw (0.55,1.8) ellipse (0.45 and 0.45);
 \draw (-2.6,-0.7) ellipse (0.45 and 0.45) (3.7,-0.8) ellipse (0.45 and 0.45);
 \draw (-0.7,-0.28) ellipse (0.45 and 0.45) (2.1,-0.35) ellipse (0.9 and 0.45);
\end{tikzpicture}
\hspace{5mm}\includegraphics[width=15mm]{/home/janlo/Documents/papers/ideologylandscapes/r/colorbar}
\end{center}
\begin{tabular}{ccccc}
 & France & Germany & Sweden & United Kingdom \\
\rotatebox{90}{Left/Right} & \includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_lrscale_2012_FR} &
\includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_lrscale_2012_DE} &
\includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_lrscale_2012_SE} &
\includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_lrscale_2012_GB}  \\
\rotatebox{90}{\ \ \ EU} & \includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_euftf_2012_DE} &
\includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_euftf_2012_FR} &
\includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_euftf_2012_SE} &
\includegraphics[trim=40 40 40 40,width=0.185\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/r/ess-hist_euftf_2012_GB}
\end{tabular}
\caption{Stylized facts of political landscapes demonstrated for the left-right selfplacement in France 2012. The same landscape for Germany, Sweden and the United Kingdom and for all four countries the landscape of opinions about European integration. All data from ESS 2012 using the design weights. (Variables \texttt{lrscale} and \texttt{euftf} using \texttt{dweight} as weight.)}\label{fig:ess}
\end{figure}

\textcite{Lorenz2015ModelingEvolutionIdeological} tries to reproduce these stylized facts with the bounded confidence model in the version with dyadic communication as in \textcite{Deffuant.Neau.ea2000MixingBeliefsamong} including random opinion replacements (called ``reconsideration'' by \textcite{Lorenz2015ModelingEvolutionIdeological}), operationalized as noise following \textcite{Pineda.Toral.ea2009Noisycontinuous-opiniondynamics}. 

In short, the model is about $N$ agents (e.g. $N=1000$) which opinions are numbers from the interval $[0,1]$, which represent a continuous version of the 11-point scale from the ESS.\footnote{Continuous datapoints can be linked back to the discrete format format of the ESS through discretization analog to \textcite{Lorenz2009Universalityofmovie}.} Initially, the opinion of an agent is a random draw from a uniform distribution on the interval $[0,1]$. The opinions of agents are subject to change over time because they communicate with others. All agents are further equipped with a homogeneous \emph{bound of confidence} $\varepsilon$ also from the range [0,1] which is not subject to change but determines the maximal ideological distance to others which an agent is willing to take into account to revise its ideological position. 
We consider agents to change their ideological positions because of two processes: \emph{adaptation under bounded confidence} and \emph{random opinion replacement}. 
For each agent $i$ and each time step $t$ opinion adaptation happens with probability $1-m$ and opinion replacement happens otherwise. In the case of adaptation, another agent $j$ is picked at random. Agent $i$ interacts with it only when their distance in opinion is less than its bound of confidence $|x_i(t) - x_j(t)| < \varepsilon$. If agent $j$ is close enough agent $i$ changes its ideological position to the average of the two positions $x_i(t+1) = (x_i(t) + x_j(t))/2$. In case of opinion replacement, agent $i$ does not engage in interaction with others but reconsiders its ideological position from scratch. That means it chooses a new random value from the interval $[0,1]$. \footnote{Remark: Dynamics on the macrolevel remains unchanged if instead of picking a new random opinion the agent just switches back to its initial opinion, which was picked at random only at the beginning of the simulation.}
Some care is necessary with respect to specifics of the simulation algorithm and model parameters. In many implementation, agents $i$ and $j$ are drawn at the time and both of their opinions are updated. Under this mode, there are on average twice as much adaptation events per agent than in the model described here where only $i$ adapts. As a consequence, the model with bilateral adaptation behaves essentially as the model with unilateral adaptation but only half of the probability for opinion replacement, while dynamics of the former are twice as fast as dynamics of the latter. Here we use unilateral adaptation, while in \textcite{Lorenz2015ModelingEvolutionIdeological} bilateral adaptation was used. Further on, \textcite{Lorenz2015ModelingEvolutionIdeological} uses $p$ for the probability of random opinion replacement, while we use $m$ here for consistency with \textcite{Pineda.Toral.ea2009Noisycontinuous-opiniondynamics}. It thus holds $m = p/2$ in comparison of the results here with the results in \textcite{Lorenz2015ModelingEvolutionIdeological}.

The model is implemented in NetLogo \parencite{Wilensky1999NetLogo} and made available by \textcite{Lorenz2017Sourcefilesand}. Figure~\ref{fig:snap1} shows the full screen shot of the ffmodel. 
The main panel in the center shows the trajectories of opinions up to the current time step (=\texttt{tick}) moving from left to right. For longer runs this panel turns into a ``roling'' display showing only the latest time steps. 
At the bottom there are two observers which accompany the central panel of trajectories. 
The left hand side panel shows the histogram of the current opinions in eleven equidistant bins, resembling the discretization of empirical opinion landscapes from the ESS. 
The right hand side panel shows the evolution of the mean and the median opinion. 

\begin{sidewaysfigure}
 \includegraphics[width=\textwidth]{figs/Screenshot.png}
 \caption{Full screen of the NetLogo model provided by \textcite{Lorenz2017Sourcefilesand}. A run is shown for 500 agents with homogeneous bound of confidence $\varepsilon=0.25$ and without random opinion replacement ($m=0$).}\label{fig:snap1}
\end{sidewaysfigure}

To understand model dynamics, let us first look at homogeneous bounds of confidence without random opinion replacement ($m=0$). 
Large bounds of confidence (approximately $\varepsilon>0.27$) lead to \emph{consensus}. Intermediate bounds of confidence (approximately $0.18<\varepsilon<0.27$) lead to \emph{polarization} into two equally large opinion clusters, one moderately left-wing and one moderately right-wing. Small bounds of confidence ($\varepsilon<0.18$) lead to \emph{plurality} of three or more clusters. Fig.~\ref{fig:homo} shows trajectories for different bounds of confidence to demonstrate consensus, polarization and plurality. These can also be checked in the NetLogo model through example buttons (cf.~Fig.~\ref{fig:snap1}).

\begin{figure}
\begin{center}
\begin{tikzpicture}
 \draw[->] (0,0) node[anchor=south west]{\includegraphics[viewport=0 0 133 190,clip,width=0.29\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/netlogo/homo1}}   --(2,0)node[below=2pt]{\sf time}node[above=5.5cm]{$\varepsilon=0.15$}-- (3.7,0);
 \draw[->] (0,0) -- (0,2.75) node[rotate=90,anchor=west,above=2pt]{\sf opinion} -- (0,5.5);
\end{tikzpicture}
\begin{tikzpicture}
 \draw[->] (0,0) node[anchor=south west]{\includegraphics[viewport=0 0 133 190,clip,width=0.29\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/netlogo/homo2}}  --(2,0)node[below=2pt]{\sf time}node[above=5.5cm]{$\varepsilon=0.25$}-- (3.7,0);
 \draw[->] (0,0) -- (0,5.5);
\end{tikzpicture}
\begin{tikzpicture}
 \draw[->] (0,0) node[anchor=south west]{\includegraphics[viewport=0 0 133 190,clip,width=0.29\textwidth]{/home/janlo/Documents/papers/ideologylandscapes/netlogo/homo3}}  --(2,0)node[below=2pt]{\sf time}node[above=5.5cm]{$\varepsilon=0.3$}-- (3.7,0);
 \draw[->] (0,0) -- (0,5.5);
\end{tikzpicture}
\end{center} 
\caption{Trajectories for $N=500$ agents with homogeneous $\varepsilon$ without random opinion replacement leading to plurality (left, $\varepsilon=0.15$), polarization (center, $\varepsilon=0.25$), and consensus  (right, $\varepsilon=0.3$).}\label{fig:homo}
\end{figure}

Compared to the stylized facts, the model without random opinion replacement is able to produce clustered opinion landscapes, but clusters are maximally pronounced peaks and not blurred peaks as we see them in the empirical opinion landscapes of the ESS data. 
The introduction of random opinion replacement ($m>0$) blurs the peakedness of clusters such that they visually come closer to the opinion landscapes observed empirically in the ESS data. 

Fig.~\ref{fig:reconsider} shows three example trajectories in heatmap visualization and a snapshot of the histogram. As can be inferred from the trajectories, the histogram fluctuates but keeps its typical shape. The values $\varepsilon=0.15,0.25,0.3$ are taken from Fig.~\ref{fig:homo} and the values $m=0.06,0.1,0.045$ are chosen in an attempt to visually match some characteristics of ESS opinion landscapes. 

\begin{figure}\sf
 \textbf{A:} $\varepsilon=0.15$, $m=0.06$, $N=1000$\\[1mm]
 \includegraphics[width=0.56\textwidth]{figs/bc_lrscale_pv1.png}
 \includegraphics[width=0.42\textwidth]{figs/bc_lrscale_ph1.png}\\[3mm]
 \textbf{B:} $\varepsilon=0.25$, $m=0.1$, $N=1000$\\[1mm]
 \includegraphics[width=0.56\textwidth]{figs/bc_lrscale_pv2.png}
 \includegraphics[width=0.42\textwidth]{figs/bc_lrscale_ph2.png}\\[3mm]
 \textbf{C:} $\varepsilon=0.3$, $m=0.045$, $N=1000$\\[1mm]
 \includegraphics[width=0.56\textwidth]{figs/bc_lrscale_pv3.png}
 \includegraphics[width=0.42\textwidth]{figs/bc_lrscale_ph3.png}
\caption{Trajectories of $N=1000$ agents with closed-minded (A), intermediate (B), and open-minded (C) bounds of confidence and different probabilities of random opinion replacement $m$. The bounds of confidence are the same as in Fig.~\ref{fig:homo}.}\label{fig:reconsider}
\end{figure}

To what extent do opinion landscapes from the model in Fig.~\ref{fig:reconsider} match empirical opinion landscapes in Fig.~\ref{fig:ess}? Generally, no model specification is able to reproduce all stylized facts from empirical distributions. Thus, the development of a parameter fitting procedure is left for future models which can reproduce all stylized facts. The further discussion is exploratory and qualitatively. Nevertheless, the model distributions match the empirically observed distributions visually much better than any specification of a beta distribution or other standard distributions. As the model has also only two parameters $(\varepsilon,p)$ this is already a small success. 

Going along increasing bounds of confidence in Fig.~\ref{fig:reconsider}, different stylized facts of empirical distributions are reproduced by the model. 

\emph{Closed-minded agents} (Fig.~\ref{fig:reconsider}A $\varepsilon=0.15$): Under a relatively high probability of reconsideration ($m=0.12$) an opinion landscape with three peaks is produced -- a central, a moderate left, and a moderate right cluster as we see it in many empirical landscapes; but empirical landscapes of this type usually also have extremal peaks, the moderate peaks lie a bit closer to the center and never on the bins 1 and 9 and the central peak is much more pronounced.

\emph{Intermediate agents} (Fig.~\ref{fig:reconsider}B $\varepsilon=0.25$): Under a high probability of random opinion replacement ($m=0.2$) the opinion landscape is still polarized as for the case $m=0$, but clusters are blurred such that their shape and location at bins 2 and 3 (respectively 7 and 8) matches moderate off-center empirical clusters. In this specification the empirically observed dominant central cluster misses completely. If it would evolve it would absorb the moderate clusters quickly. Also extremal peaks as empircally found are missing. 

\emph{Open-minded agents} (Fig.~\ref{fig:reconsider}C $\varepsilon=0.3$): Under an intermediate probability of random opinion replacement ($m=0.09$) the opinion landscape shows a large central peak as observed empirically, also small peaks at the extremes match the empirical landscapes, although extremal peaks lie empirically more pronounced at 0 and 10 and not on bin 0 and 1 (respectively bin 9 and 10) as in the model. The weak point here is that empirically we observe much more mass on the bins close to the center. 

In conclusion, several stylized facts can be reproduced by this two parameter model much better than classical distributions can, but the model can not produce a large central peak, and tendencies for small and blurred moderate off-center peaks and small or tiny extremal peaks at the same time in its current form. 

Although there has been pioneering work by \textcite{Troitzsch1987BuergerperzeptionundLegitimierung},
agent-based models for the evolution of ideological landscapes are still in its infancy and it remains to show if they can add interesting insight to political dynamics. At least the possibility for counterfactual analysis at critical values seems promising for studying and understanding self-driven abrupt changes in political landscapes. 

%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 2, TOPIC 5, Opinion Pattern Diagrams %%%%%%%%%%%%%%%%%%%

\section{Systemic effects in opinion pattern diagrams} \label{sec:opd}

 In the analysis of the four models in Sections \ref{sec:OD1} to \ref{sec:OD4} we focused at some point on one particular parameter in each. 
 These were the diffusivity of information $\Omega$ under a constant rate of inter- versus intra-community links \parencite{Shin.Lorenz2010TippingDiffusivityin}, the interest in politics $q$ \parencite{Metz.Lorenz2013Becomewhoyou}, the fitted movie quality $\mu$ \parencite{Lorenz2009Universalityofmovie}, and the bound of confidence $\varepsilon$ \parencite{Lorenz2015ModelingEvolutionIdeological}. 
 All models show under some conditions a typical distribution of the agents' dynamic opinion variable for each values of the parameter. 
 We can track the shape of this typical distribution over the range of the parameter and construct \emph{opinion pattern diagrams}. 
 These diagrams show systemic effects visually.
 In particular, they visualize four different mechanisms for bipolarization in opinion landscapes. 
 The methods to compute the opinion pattern diagrams are documented in the source files of this thesis \parencite{Lorenz2017Sourcefilesand}.

\begin{figure}[htb]
<<ODP-tipping, echo=FALSE, fig.height=4>>=
Delta <- 0.2
init_y <- c(0.5,-0.4)
FAC <- seq(0.1,3,by=0.02)
if (!file.exists("R/ODP_ias.RData")) {
  R <- matrix(nrow = length(FAC), ncol = 2)
  for (i in 1:length(FAC)) R[i,] <- fix_ias(init_y,FAC[i],Delta)
  save(FAC,R,file = "R/ODP_ias.RData")
}
load(file = "R/ODP_ias.RData")
M <- tbl_df(cbind(c(FAC,FAC),c(R)))
names(M) <- c("fac","y")
ggplot(M,aes(fac,y)) + annotate("rect", xmin = 0.1, xmax = 3.02, ymin = -1, ymax = 1, fill="#3288BD") +
  stat_bin2d(binwidth = c(0.01999, 0.02), drop=FALSE) + ylim(-1,1) +
  scale_fill_distiller(palette = "Spectral", limits=c(0, 2)) +
  labs(y="y (stable opinions)", x=expression(
    "Diffusivity"~Omega~"(in units of"~Delta~"with"~Omega[X]==0.25*Omega~"and"~Omega[0]==Omega~")")) +
  annotate("point", x=c(0,0), y=init_y, shape=21, fill="#FFFFBF", color="black", size=3) + 
  annotate("text", x=c(0.1,0.1), y=init_y, label = c('y[1] (initial)','y[2] (initial)'),hjust = 0, parse=TRUE) +
  theme(legend.position = "none")
@
\caption{Opinion pattern diagram of an information accumulation system with tipping diffusivity \parencite{Shin.Lorenz2010TippingDiffusivityin} following increasing diffusivity under a constant ratio of inter- vs. intra-community diffusivity analog to the red arrow in Figure \ref{fig:ShinLorenz}. }\label{fig:opdias}
\end{figure}

 Figure \ref{fig:opdias} shows the attractive fixed points of the information accumulation system with two equally sized groups when one group starts with opinion $y_1=\Sexpr{init_y[1]}$ and the other group with opinion $y_2=\Sexpr{init_y[2]}$. 
 The diffusivity parameters are set such that $\Omega$ is the intra-community diffusivity ($\Omega_0$) and the inter-community diffusivity ($\Omega_X$) is a fourth of it. 
 This models a situation where each agent has four times more links to the own community than to the other. Total diffusivity increases with $\Omega$ in units of the rate of forgetting $\Delta$. 
 The opinion pattern diagram shows the two important structural changes (called bifurcations in the theory of dynamical systems): 
 (1) At diffusivity being more than 0.8 of the rate of forgetting ($\Omega=0.8\Delta$) the consensual fixed points starts to deviate from zero towards the side of the group which starts with the initially more extreme opinion. 
 (2) At diffusivity being 2.25 times larger than the rate of forgetting ($\Omega=2.25\Delta$) the consensual fixed point departs into a bipolarized one, where both groups can maintain different opposing opinions. 
 Note, that the bipolarized situation only sustains itself when initial differences in opinions between communities are strong enough. 
 In this case, the higher intercommunity diffusivity is strong enough to sustain against forgetting and attraction of the other community's opinion through inter-community diffusivity.  
 
\begin{figure}[tb]
<<OPD-partyID, fig.height=2.5, echo=FALSE>>=
n <- 10
num_attach <- 0:n
Q <- seq(0,1,length.out = 51)
M <- matrix(NA, nrow = length(num_attach), ncol=length(Q))
for (i in 1:length(Q)) {
  M[,i] <- choose(n,num_attach) * beta(num_attach + Q[i], n - num_attach + 1-Q[i])/beta(Q[i],1-Q[i])
}
R <- reshape2::melt(M)
names(R) <- c("number of attachments","interest in politics","frequency")
R$`number of attachments` <- num_attach[R$`number of attachments`]
R$`interest in politics` <- Q[R$`interest in politics`]
ggplot(filter(R), aes(`interest in politics`, `number of attachments`)) +
  geom_raster(aes(fill=frequency)) + scale_fill_distiller(palette = "Spectral", na.value = "maroon", limits=c(0,0.33)) +
  labs(x="q (Interest in Politics)") + scale_y_continuous(breaks=seq(0,10,by=1))
@
\caption{Opinion pattern diagram of the distribution of the number of party attachments after ten rounds with respect to the interest in politics $q$ as modeled by \textcite{Metz.Lorenz2013Becomewhoyou}.}\label{fig:opdparty}
\end{figure}

 The model of party attachment through self-reinforcing \parencite{Metz.Lorenz2013Becomewhoyou} produces an even more bipolarized situation as shown in Figure \ref{fig:opdparty}. 
 Individuals rarely stay homing in and out of party attachment such that they are ``intermediately attached'' to a party.
 Most people support a party either almost always or almost never. 
 This creates an always U-shaped distribution with two peaks, both at the extremes. 
 The relative size of the two extremal peaks changes with the interest in politics $q$. 
 Low interest in politics creates a larger peak of not attached people and high interest a larger peak of attached people, but even when the interest in politics is intermediate there are few moderately attached people.  The bipolarization is created through self-reinforcement. Whenever a person has a slightly higher (or slightly lower) number of attachments (or non-attachements) it is likely that this imbalance will increase. 
 
\begin{figure}[htb]
<<ODP-ratings, echo=FALSE, fig.height=3>>=
MU <- seq(2,10,by=0.25)
M <- matrix(NA, nrow = length(x), ncol=length(MU))
for (i in 1:length(MU)) M[,i] <- confinedLevySkewAlphaStablePdf(MU[i])
R <- reshape2::melt(M)
names(R) <- c("stars","mu","frequency")
R$star <- num_attach[R$stars]
R$mu <- MU[R$mu]
ggplot(filter(R), aes(mu, stars)) + geom_raster(aes(fill=frequency)) + 
  scale_fill_distiller(palette = "Spectral", limits=c(0, 0.33), na.value = "maroon") + scale_y_continuous(breaks=1:10) +
  labs(x=expression(mu-"fitted (Movie Quality)"))
@
\caption{Opinion pattern diagram of a movie rating histogram with respect to the empirically calibrated $\mu$-fit of \textcite{Lorenz2009Universalityofmovie}.}\label{fig:opdmovies}
\end{figure}

 The model of movie rating histograms in \textcite{Lorenz2009Universalityofmovie} also shows bipolarization with two peaks at the extremes for very good and very bad movies. 
 For intermediate movies these extremal peaks are outshined by a large central peak. 
 Figure \ref{fig:opdmovies} shows the distribution of movie ratings from the confined and discretized Levy-skew $\alpha$-stable distribution with three parameters calibrated by $\mu$ as shown by the green lines in Figure \ref{fig:LevyParams}.
 The bipolarization in this model is created through confinement and thus different to the former two models. 

\begin{figure}[htb]
<<OPD-boundedconfidence, echo=FALSE, fig.height=3.5>>=
E <- seq(0.1,0.4,by=0.002)
d <- 101 # discretization of opinion space
P_init <- rep(1/d,d) # initial distribution
Ops = seq(0,1,length.out = d) # labels of opinions
m <- 0.1
filename <- paste0("R/bc_opinion_pattern_diagram_m",m,".RData")
if (!file.exists(filename)) {
# Computation takes several HOURS!!! when R/bc_opinion_pattern_diagram_m[m] has to be built anew
  makeM(m, P_init, E, prec=10^-14)
}
load(filename)
R <- reshape2::melt(M)
names(R) <- c("opinion","confidencebound","frequency")
R$opinion <- Ops[R$opinion]
R$confidencebound <- E[R$confidencebound]
ggplot(R, aes(confidencebound, opinion)) + 
  geom_raster(aes(fill=frequency)) + scale_fill_distiller(palette = "Spectral") +
  labs(x = expression(paste(epsilon," (Bound of Confidence)"))) + ggtitle(paste0("Opinion pattern diagram for m = ",m))
@
\caption{Opinion pattern diagram of the evolving stable opinion landscape of the bounded confidence model \parencite{Lorenz2015ModelingEvolutionIdeological} starting from a uniform distribution of opinions with respect to the bound of confidence $\varepsilon$ and a constant rate of random opinion replacement $m=0.1$. Stable opinion landscapes for each $\varepsilon$ computed with interactive Markov chains \parencite[cf.~][]{Lorenz2005Continuousopiniondynamics,Lorenz2007ContinuousOpinionDynamics}.
}   \label{fig:opdbc}
\end{figure}

 Finally, the opinion pattern diagram of the bounded confidence model with moderate random opinion replacement ($m=0.1$) in Figure \ref{fig:opdbc} shows another mechanism which can lead to bipolarization. 
 For intermediate bounds of confidence $0.19 < \varepsilon < 0.27$ we see two big off-central clusters mimicking a left-wing and a right-wing political camp. 
 In this regime there as very few people with centrist opinions. 
 The bipolarization here is not created through reinforcement away from a natural ground state as in Figure \ref{fig:opdias}, not through reinforcement towards one of the two extremes as in Figure \ref{fig:opdparty}, and not through overshooting and confinement as in Figure \ref{fig:opdmovies}, but only through contractive forces among opinions which act only when these are close enough. 
 For large bounds of confidence the contractive forces are strong enough to create a consensual central cluster. 
 At $\varepsilon = 0.27$ there is a sharp transition to the bipolarized state. For $\varepsilon\approx 0.19$ there is a smooth transition towards a three cluster state where a central cluster emerges again. 
 
 The sharpness and smoothness of transitions between regimes of different numbers of clusters depends on the rate of random opinion replacement $m$. 
 Figure \ref{fig:opdbc2} shows that the opinion pattern diagram for $m=0.045$ has three sharp transition between the regimes of one, two, three and four main clusters, while the opinion pattern diagram for $m=0.12$ has only smooth transitions.  
 
\begin{figure}[htb]
<<OPD-boundedconfidence2, echo=FALSE, fig.height=2.4>>=
m <- 0.045
filename <- paste0("R/bc_opinion_pattern_diagram_m",m,".RData")
if (!file.exists(filename)) {
# Computation takes several HOURS!!! when R/bc_opinion_pattern_diagram_m[m] has to be built anew
  makeM(m, P_init, E, prec=10^-14)
}
load(filename)
R <- reshape2::melt(M)
names(R) <- c("opinion","confidencebound","frequency")
R$opinion <- Ops[R$opinion]
R$confidencebound <- E[R$confidencebound]
g1 <- ggplot(R, aes(confidencebound, opinion)) + 
  ggtitle(paste0("m = ",m)) +
  geom_raster(aes(fill=frequency)) + scale_fill_distiller(palette = "Spectral") +
  labs(x = expression(paste(epsilon," (Bound of Confidence)"))) + theme(legend.position = "none")
m <- 0.12
filename <- paste0("R/bc_opinion_pattern_diagram_m",m,".RData")
if (!file.exists(filename)) {
# Computation takes several HOURS!!! when R/bc_opinion_pattern_diagram_m[m] has to be built anew
  makeM(m, P_init, E, prec=10^-14)
}
load(filename)
R <- reshape2::melt(M)
names(R) <- c("opinion","confidencebound","frequency")
R$opinion <- Ops[R$opinion]
R$confidencebound <- E[R$confidencebound]
g2 <- ggplot(R, aes(confidencebound, opinion))  + 
  ggtitle(paste0("m = ",m)) + 
  geom_raster(aes(fill=frequency)) + scale_fill_distiller(palette = "Spectral") +
  labs(x = expression(paste(epsilon," (Bound of Confidence)"))) + theme(legend.position = "none")
grid.newpage()
grid.draw(cbind(ggplotGrob(g1),ggplotGrob(g2), size="first"))
@
\caption{Opinion pattern diagrams analog to Figure \ref{fig:opdbc} but with $m=0.045$ and $m=0.12$.}   \label{fig:opdbc2}
\end{figure} 
 
 
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% CHAPTER 3 %%%%%%%%%%%%%%%%%
\chapter{Societal Growth}\label{ch:growth}
\mbox{}\\[-1cm]
{\footnotesize
Papers of the Habilitation Work: \\[-1.5mm]
\begin{compactenum}
\setcounter{enumi}{6}
\item \fullcite{Koenig.Lorenz.ea2016Innovationvsimitation}
\item \fullcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth}
\item \fullcite{Lorenz.Paetzel.ea2016JustDontCall}
\end{compactenum}
}
\vspace{1cm}

 Growth (as well as decline) is fundamental to all living organisms. 
 This includes human societies. 
 Societal growth is the growth of a societal value such as total wealth or total productivity in a society composed of individuals which act mostly independent but also interrelated through interaction or societal institutions. 
 Whether a society or economy experiences societal growth or decline, can critically depend on this coupling of individual growth process. 
 This chapter presents two examples of this kind of effect. 
 \textcite{Koenig.Lorenz.ea2016Innovationvsimitation} show how strategies of innovation together with strategies of imitation of technology optimize productivity growth in an economy of independent firms. 
 \textcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth} show how redistribution of wealth through proportional taxation and redistribution can spur growth. 
 The results pose the question if and how a democratic society is capable of realizing the desired levels of redistribution. 
 The chapter continues with an experimental study of democratic decisions about redistribution showing a large framing effect, and concludes with a discussion of systemic growth effects. 

 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 3, TOPIC 1, Innovation and Imitation %%%%%%%%%%%%%%%%%%%
\section{How innovation and imitation drive growth of productivity} 

 Productivity of a firm is its efficiency of production. It can be measured for a business year e.g. as total factor productivity estimated based on the balance sheet items value added, fixed assets (capital), number of employees (labour), and costs of materials using the method of \textcite{Levinsohn.Petrin2003EstimatingProductionFunctions}. \textcite{Koenig.Lorenz.ea2016Innovationvsimitation} presents a model which reproduces the evolution of the distribution of productivity remarkably well. The model shows that imitation can ``push'' the distribution forward, leading to higher growth rates than relying on innovation alone. 
 
 Figure \ref{fig:ODEFR} shows the distribution of the total factor productivity of a balances panel of French firms for the years 1995 to 2003 in a double logarithmic (log-log) plot. Results are structurally similar for other countries and for unbalanced panels. 

 The main stylized facts found in the data are (cf. Section B.2 of the Supplementary Material of \textcite{Koenig.Lorenz.ea2016Innovationvsimitation} for some numbers)
 \begin{compactenum}[(i)]
  \item the distribution of productivity is dispersed spanning some orders of magnitude,
  \item the distribution of high-productivity firms follows a power law $P(A)\propto e^{-\lambda A}$ (visible as a straight line in the log-log plot, the exponent is on average $\lambda=3.73$ with little variation over the years), 
    \item the distribution of low-productivity firms follows a power law $P(A)\propto e^{\rho A}$ (the exponent is on average $\rho=2.36$ with little variation over the years), and 
  \item over time the distribution is a traveling wave, which means that its shape stays constant while it shifts sideways by a value $\nu$ per year.\footnote{Because of the logarithmic x-axis the visible linear additive shift corresponds to a multiplicative one. In the following we will look at dynamics of log-productivity and use a normal linear x-axis and a logarithmic y-axis, which is identical.} The shift is $\nu = 0.0271$ based an a linear regression of the mean of log-productivty on time .
\end{compactenum}

\begin{figure}[tbp]
 \centering%
 \includegraphics[width=0.6\textwidth]{figs/fig_ODE_tfpFR}
 \includegraphics[width=0.3\textwidth]{figs/fig_nu_fitFR}
 \caption{The panel on the left side shows the total factor productivity (TFP) distribution of French firms over the years from 1995 (blue) to 2003 (red). The panel on the right side shows the mean and standard
deviation of the $\log$-TFP, with fitted regression lines.}
\label{fig:ODEFR}
\end{figure}


 To model the evolution of the productivity distribution in an economy we consider that firms strive to increase their productivity and have access to two strategies from which they can choose one in a business year: innovation or imitation of the technologies of others. 
 To that end, we consider that productivity of a firm evolves by taking steps on a quality ladder with rungs spaced proportionally by a factor $\bar{A}$.\footnote{Proportional spacing means, by climbing on the next rung the productivity of a firm increases by a constant factor. As a consequence, the log-productivity ladder has rungs spaced linearly with a constant additive increment.}
 Let us formulate both strategies as probabilistic upward movements on this quality ladder. 
 A firm pursuing the \emph{innovation strategy} moves one rung upwards with probability $p$.
 A firm pursuing the \emph{imitation strategy} selects another firm at random and if this firm has a higher productivity it tries to successively move steps upwards towards this level. It succeeds at each step with probability $q$ and the move ends after a failure. 
 A natural assumption is that the success probability for imitation is higher than for innovation $q>p$. 
 A good guess for realistic numbers (based on the results to be presented) is $p=0.01$ and $q=0.1$, meaning that one out of a hundred attempts to innovate technologies and one out of ten attempts to copy technologies is a success. 
 For a relatively low $q$ the probability to make two or more steps towards a better firm is negligible. 
 
 The probability for innovation is independent of the firm's current productivity as well as of the productivities of others. 
 The probability to improve productivity through imitiation instead depends also on the probability to find a better firm, which depends on the position of the own productivity in the current distribution. 
 When many firms are better this probability is higher. 
 When no firm is better it is zero. 
 
 Let us briefly discuss the dynamics of the distribution for both strategies in isolation. 
 When firms only use innovation then log-productivity would create a binomial distribution with increasing $n$ over time. Thus, log-productivity would diffuse with an upward drift. 
 That means, the expected value of log-productivity increases with $p$ while the standard deviation increases with the square root in time. 
 Dispersion of productivity would continue to increase contrary to the stylized facts. 
 If firms only imitate then the best firm can never improve while all other firms would come closer and closer to it over time. 
 Finally, all firms would end up with the same productivity which contradicts the dispersion and the upward movement from the stylized facts. 
 
 Let us consider that a firm can decide which strategy to use based on the expected gain in productivity. 
 For innovation the expected gain is always $p$. 
 For imitation instead it is at least $q$ times the fraction of firms which are better.\footnote{This neglects the possibility that an imitating firm jumps more than one rung at a time.}
 When $q>p$ there is thus for each distribution of productivity a threshold above which firms benefit more from innovation than from imitation while it is the other way round for firms below the threshold. 
 The dynamics of the distribution of productivity based on this endogeneous choice of the strategy can be described by a system of ordinary differential equations which solutions can be analysed and numerically computed \parencite{Koenig.Lorenz.ea2016Innovationvsimitation}. 

\begin{figure}[tbp]
 \centering%
 \includegraphics[width=\textwidth]{figs/fig_examplesQ-eps-converted-to}
 \caption{Examples of numerical solutions of the system of ODEs with different values of $q.$ In
all cases it is $\log \bar{A}=1$ and $p=0.1.$ The top left panel shows an
economy where growth is driven by innovation only ($q=0$). The top right
panel shows the case in which $p=q=0.1.$ The bottom panels show,
respectively, the case of $q=0.2$ and $q=0.5.$}
\label{fig:sdf}
\end{figure}

 It turns out that for any non-degenerate\footnote{Degenerate distributions could be some with extremely fat tails or with multiple modes with large gaps} initial distributions the solutions evolve towards the same characteristics which parameters depend on the magnitudes of $p$ and $q$. 
 Figure \ref{fig:sdf} show an example for this evolution for $p=0.1$. 
 The figure shows that for $q\leq p$ indeed log-productivity diffuses following a binomial distribution. Note, that the binomial distribution mimics the normal distribution on a discrete domain. The normal distribution shows the shape of a parabola when the y-axis is logarithmized. 
 This is the case for the upper two panels in Figure \ref{fig:sdf}. 
 If $q$ is much larger than $p$ the solution is a traveling wave with speed $\nu$ and a stable shape characterized by power laws on both sides with potentially different exponents $\lambda$ and $\rho$ which matches the stylized facts to a remarkable degree. 

\begin{figure}[tbp]
\centering
\includegraphics[width=0.32\textwidth]{figs/fig_contourlambdaIbeta4} 
\includegraphics[width=0.32\textwidth]{figs/fig_contournuIbeta4} 
\includegraphics[width=0.32\textwidth]{figs/fig_contourrhoIbeta4} 
\caption{Exploration of impact of innovation probability $p$, imitation
probability $q$, on the dependent power-law parameters $\lambda$,
and $\rho$, and on the productivity growth rate $\nu$. The
contour plots are based on numerical computation of solutions of the system
of ODEs. The black dot mark the calibrated $(p,q)$-point for France. ($\beta=100$ in the subfigure titles refers to parameter, only discussed in \textcite{Koenig.Lorenz.ea2016Innovationvsimitation}.)}
\label{fig:contour_pq}
\end{figure}

 Figure \ref{fig:contour_pq} explores the impact of the model parameters $p$ and $q$ on the output parameters: the power law exponents $\lambda$ and $\rho$ and the growth rate of productivity $\nu$. 
 
\begin{figure}[tbp]
 \centering%
 \includegraphics[width=0.7\textwidth]{figs/fig_ODEsimple_tfpFR}
\caption{Comparison of the empirical distributions of Figure \ref{fig:ODEFR} with 
the calibrated model ($p=0.0049$ and $q=0.106$) for the
years $1995$, $1999$ and $2003$. The empirical productivity values have been
binned to produce the histogram shown in the figure, using $11$ bins across
all observed productivity values.}
\label{fig:fit}
\end{figure}

 Figure \ref{fig:fit} shows the data for the French firms (for better visual display only for three years) and
the trajectory of the model with parameters $p=0.0049$ and $q=0.106$ which were calibrated to best fit with respect to $\lambda, \rho,$ and $\nu$.

 Thus, the model reproduces the stylized facts. It shows that the interplay of imitation and innovation is what drives the growth of productivity. 

%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 3, TOPIC 2, Redistribution spurs Growth %%%%%%%%%%%%%%%%%%%
\section{How redistribution can spur growth}

 \textcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth} study how the sum of the the human capital of individuals grows or declines in a society of economic actors which have to redistribute capital. 
 At first sight, human capital seems to be not redistributable, but using the concept of ``risky human capital''\parencite{Grochulski.Piskorski2010Riskyhumancapital} presents human capital as the result of risky investments and stochastic depreciation. 
 As investment and returns are through labour and ultimately monetary capital it can be redistributed in this form. 
 The model also generalizes to other sorts of capital and wealth. 
 
 We model the individual random growth or decline of human capital as proportional to the individual endowment of income. 
 With respect to knowledge as a part of human capital this means that the creation (or destruction) of new knowledge is proportional to the old level of knowledge. 
 
 Following \textcite{Meltzer.Richard1981RationalTheoryof}, we couple agents by fully redistributive taxation. 
 That means collected taxes are redistributed in lump sums of equal size. 
 We associate the redistribution with efficiency losses following \textcite{Persson.Tabellini1994IsInequalityHarmful}. 
 We treat three tax regimes: proportional, maximally progressive, and maximally degressive taxation. 

 For \emph{proportional taxation} the dynamic equation of a society's vector of incomes $y(t) = (y_1(t), \dots, y_N(t))$  is 
 \begin{equation}
  y(t+1) = \underbrace{(1-a)\eta(t) y(t)}_\text{\parbox{25mm}{\centering income after stochastic \\[-1.5mm] growth and tax}} + \underbrace{(1-b)a \frac{1}{N}\sum_{i=1}^N \eta_i(t) y_i(t)}_\text{\parbox{33mm}{\centering lump sum from tax revenue \\[-1.5mm] with efficiency loss}} \label{eq:tax}
 \end{equation}
where $a$ is the \emph{tax rate}, $b$ the \emph{inefficieny factor}\footnote{The inefficiency factor is called ``admin rate'' by \textcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth}.}, and $\eta(t)$ a vector of realizations from independent and identically distributed random variables which are the stochastic growth rates for all individuals. The values $\eta_1(t),\dots,\eta_2(t)$ are the random growth rates. They are positive numbers standing for decline when below one and for growth when above one. Note, that the product $\eta(t) y(t)$
stands for component-wise multiplication of two vectors here.  
 
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{figs/fig1-crop}
\end{center}
\caption{Demonstration of redistribution through proportional, regressive, and progressive tax. Six individuals with incomes $y = [100,300,600,1000,1500,2100]$. All tax functions are such that $a = 1/3$ of the total income $Y = 5600$ is taxed. The inefficiency factor is set to $b=1/4$. (Thus, the fee for maximally regressive tax is 366.66 and the maximal income for maximally progressive taxation is 911.11.)}
\label{fig:redis}
\end{figure} 

 For \emph{maximally progressive taxation} we compute in each step a maximal income above which everthing is taxed such that the tax revenue equals a fraction of $a$ from the total income $Y(t) = \sum_{i=1}^Ny_i(t)$. 
 For \emph{maximally regressive taxation} we compute in each step a fee which has to be paid at most by everyone such that the tax revenue equals a fraction of $a$ from the total income $Y(t)$. Figure \ref{fig:redis} sketches examplarily how the three tax regime operate on the incomes of a small society of six individuals. 
 
 Figure \ref{fig:growthex} shows trajectories of $Y(t)$ based on (\ref{eq:tax}) and the two other tax regimes for 500 time steps with 10 individuals and the same realizations of $\eta(t)$. 
 The figure shows that the progressive regime has the highest and the regressive regime the lowest average growth rate.  
 The small subfigures below show modifications for different parameters. 
 
 First, a higher inefficiency rate leads to lower average growth rates -- in this case even to decline. 
 
 Second, more surprisingly, also a lower tax rate leads to decline, although the tax revenue is much lower and consequently less total income is subject to efficiency losses. 
 The reason for the decline under low tax rates is best understood when looking at the case with no taxation.  
 No taxation would let an individual's income grow by $y_i(t+1) = \eta_i(t)y_i(t)$. 
 This kind of simple multiplicative stochastic processes have a seemingly contradictory property. 
 Even when the expected income in every next time step is larger then the current income in the long run every trajecory dies out after enough iterations. 
 Theoretically, the expected value of $Y(t)$ grows with the expected value of $\eta$ which coincides with the arithmetic mean. 
 It is the nature of multiplicative stochastic processes, that the distribution converges to be lognormally distributed with increasing variance which makes the distribution extremely skew rather quickly. 
 This has the consequence that the size of the mean depends on the existence of very few extremely rich trajectories and several others which are close to zero. 
 In any finite population there is a point in time after which very rich trajectories practically do not exist anymore because they are too unlikely. 
 As a consequence, without redistribution $Y(t)$ grows in the long run only with the geometric mean of $\eta$ which is always less than the arithmetic mean. 
 With redistribution the society can achieve growth rates closer to the arithmetic mean.
 When the geometric mean is below one and the arithmetic mean is above one, then redistribution can make the difference between extinction and prosperity of the society. 
 Redistribution re-balances gains and losses from individuals and creates thus a \emph{portfolio rebalancing effect} which mitigates the loss created by increasing skewness and finite size. 

 The third subfigure in Figure \ref{fig:growthex} shows that larger societies can achieve higher growth rates with the same level of redistribution. 
 
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{figs/fig2-crop}
\end{center}
\caption{Example trajectories for tax regimes and exemplary modifications in subfigures. Trajectories computed with the same realizations of the random variables. Black lines show limiting cases: lower line shows ``no tax'' (downward line) and ``full tax and infinite number of agents'' (upward line).}
\label{fig:growthex}
\end{figure}
 
 A closed form expression for the growth rate exists only for limiting cases. 
 Thus, a large scale systematic computer simulation has been conducted to explore the $(b,a)$-parameter space. 
 Hundred simulations were run for $t_{\max}=500$ time steps for each $(b,a)$-pair and an \emph{average growth factor} $g(b,a)$ was estimated for each time step. 
 Figure \ref{fig:growthmap} shows the growth factor $g(b,a)$ color-coded for the three tax regimes for $N=10$ and a distribution of $\eta$ with geometric mean 0.667 and arithmetic mean 1.5. 
 The figure shows that for a given rate of inefficiency $b$ there is always an intermediate optimal level taxation. 
 Moreover, for the rate of taxation can make the difference between growth and destruction. 
 The zone of growth is largest for progressive taxation and smallest for regressive taxation. 
 
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{figs/fig3-crop}
\end{center}
\caption{Zones of growth and destruction. Based on the average growth factor $g(b,a)$ for $N=10$ and $\eta$ with geometric mean 0.667 and arithmetic mean 1.5. {(A)} shows $g(b,a)$ color-coded as specified in the color axis for all three taxation schemes. Solid lines divide zones of income growth from income destruction. Dashed lines are optimal tax rates $a$ for given inefficiency rate $b$. Above the dotted line income destruction must happen. {(B)} Lines of (A) in one plot for comparison. Colors indicate tax schemes. Linestyles as above.}\label{fig:growthmap}
\end{figure}

 The reason for the existence of an optimal tax rate for a given inefficiency rate is that it balances two potentials of losses. 
 High tax rates lead to higher inefficiency, because a larger fraction of the total income is exposed to inefficiency. 
 Low tax rates weaken the portfolio rebalancing effect too much. 
 
 The results show that progressive taxation seems to be uniformly better than the other two regimes, but \textcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth} further analyze the simulation dataset under the assumption that the government is selfish and tries to maximize its gains, which are the efficiency losses of the society. 
 Under this assumption the government would increase the inefficiency rate $b$ such that its gains are optimally large. 
 The optimal inefficiency rate for the government is less then $b=1$ because too high inefficiency would lead to the destruction of the whole society. 
 The optimal $b$ is larger for progressive taxation and lowest for regressive taxation and it turns out that the average growth factor achieved for the optimal inefficiency rate for the government is highest for proportional taxation. 
 Proportional taxation is thus the better tax regime under a selfish government. 
 
 What is not part of the model is any deliberation and democratic decision on the level of redistribution. The next section explores experimentally the process of deliberation and decision about redistribution. 
 
 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 3, TOPIC 3, Tax rat vs. Minimal Income %%%%%%%%%%%%%%%%%%%
\section{How framing of a redistributive decision can change the level of redistribution}

 In democratic societies the level of redistribution is decided by the people through public deliberation and election of governments which propose redistribution policies to find majorities. 
 Public deliberation on redistribution can focus on different aspects, e.g.~on the minimal necessary income or the proportional burden to be taken by individuals to reduce  income inequality. 
 \textcite{Lorenz.Paetzel.ea2016JustDontCall} analyze whether the level of redistribution a society agrees upon can be manipulated by the way in which the issue of redistribution is presented. 
 As \textcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth} a simplified version of the Meltzer-Richard model was utilized which rests on proportional taxation providing an equal lump-sum payment for each group member \parencite{Meltzer.Richard1981RationalTheoryof}. 
 In contrast to \textcite{Lorenz.Paetzel.ea2013Redistributionspursgrowth} but in line with many other experimental works \parencite[see][]{Kittel.Paetzel.ea2015CompetitionIncomeDistribution} effects accumulating over time were not treated, but a one-shot situation. 

 We define a mechanism of redistribution for $N$ individuals with \emph{endowments} $x_1,\dots,x_N$ as
\begin{equation}
y_i = (1-\tau)\,x_i + \tau\, \bar{x},
\end{equation}
where $y_i$ is the \emph{income} of individual $i$ after redistribution under the implemented \emph{tax rate} $\tau$.\footnote{$\tau$ coincides with $a$ from, the last section. We use $\tau$ here to stay consistent with \textcite{Lorenz.Paetzel.ea2016JustDontCall}.} The average income is denoted $\bar{x} = \frac{1}{N}\sum_{j=1}^n x_i$. If instead of a tax rate a \emph{minimal income} $m$ is implemented, the necessary tax rate to achieve $m$ is computed by
\begin{equation}
\tau = \frac{m - \min_j x_j}{\bar{x} - \min_j x_j}, \label{eq:MINtaxrate}
\end{equation}
when the minimal income is within its natural bounds $\min_j x_j \leq m \leq \bar{x}$.

 Each tax rate corresponds to a minimal income and vice versa. 
 The collective decision problem to settle on $\tau$ is identical to the decision problem to settle on $m$ regarding the choice set for distributional outcomes. 
 Hence, if differences occur these differences are the result of a framing effect. 
 Henceforth, we refer to a decision about $\tau$ as the TAX frame and a decision about $m$ as the MIN frame.

 A laboratory experiment was designed to test the effect of framing the same decision differently. 
 Subjects in the experiment interacted through computer terminals. 
 They where randomly assigned to groups of five in which each subjects got randomly assigned to a certain initial endowment from a pre-specified distribution. In each session subjects had to participate in six random groups with the distributions shown in Figure \ref{fig:dists}. 
 
\begin{figure}[htb]
 \includegraphics[width=\textwidth]{figs/fig1_dists}
 \caption{Distributions of endowments used and subjects' egoistic preferences.}\label{fig:dists}
\end{figure}

 The strategic situation for rational agents with egoistic preferences is the following and highlighted in Figure \ref{fig:dists}: Individuals with endowment below average ($x_i < \bar{x}$) maximize their income through full redistribution ($\tau=100\%$), which is their rational choice under egoistic preferences. 
 Analogously, endowments above average ($x_i > \bar{x}$) lead to a preference for no redistribution ($\tau=0\%$).  Individuals with endowments being exactly $\bar{x}$ are indifferent because they will receive the average income under any tax rate. 
 The distributional conflict in the group is thus polarized except for indifferent individuals.
 In one-dimensional conflict decided through majority rule, the individual with the median preference is pivotal.  Hence under egoistic payoff maximizing preferences, any non equal distribution of endowments falls into one of three categories: a majority for full redistribution, or for no redistribution, or the median voter is indifferent. 
 
 In each of the six periods, the game had the following stages:
\begin{compactenum}
 \item \emph{Information about endowments and ideally preferred decision.} Subjects were informed about their own endowments and the endowment of all other group members. Further on, subjects were privately asked to enter their ideally preferred tax rate respectively minimum income. 
 \item \emph{Communication stage.} Each subject had to make ten proposals which appeared in a five-column table visible to all group members. The endowments were displayed throughout the whole communication stage. Subjects could only communicate through the numbers they entered to coordinate their final decisions.
 \item \emph{Collective decision stage.} After the tenth proposal, a decision box appeared where subjects had to enter their final decision privately. A group decision was achieved when at least three subjects decided for the same number (majority rule). The net income was then computed using the redistributive mechanism explained in Section 2. If the group failed to reach a collective decision, the income was either 50\% of the endowment or half of the average endowment whichever was lower.
 \item \emph{Information payoff.} Subjects were informed about the result of the collective decision and about their total net income at the end of the sixth period. The payoff in Euro was defined by a subject's average earnings over six periods with an exchange rate of 1 experimental token = \EUR{0.005}.
\end{compactenum}

\begin{figure}[htbp]
\begin{tikzpicture}
\node[text width = \textwidth, text centered] at (0,0) {
  \includegraphics[trim=0 15 20 20,clip,scale=0.6]{figs/fig2_hist_obs_tax_taxideal}
  \includegraphics[trim=0 15 20 20,clip,scale=0.6]{figs/fig2_hist_obs_min_taxideal} \\
  \includegraphics[trim=0 10 20 20,clip,scale=0.6]{figs/fig2_hist_obs_rational}
};
\path(-5.4,4.7)node{\sf \textbf{A}}+(5.7,0)node{\sf \textbf{B}}+(0.5,-5.3)node{\sf \textbf{C}};
\end{tikzpicture}
\caption{{Ideally preferred tax rates.} Histograms for the TAX (A) and the MIN (B) frame with 240 observations each. (C) shows rational egoistic preferences in the same 240 cases based on the distributions in Fig.~\ref{fig:dists}. Bin intervals are right-closed. Example: 50\% belongs to bin $(40,50]$. }\label{fig:hist_ideal}
\end{figure}

\begin{figure}[htbp]
\begin{tikzpicture}
\node[text width = \textwidth, text centered] at (0,0) {
  \includegraphics[trim=0 10 20 20,clip,width=0.49\textwidth]{figs/fig2_hist_groups_tax_taxGroupDecision}
  \includegraphics[trim=0 10 20 20,clip,width=0.49\textwidth]{figs/fig2_hist_groups_min_taxGroupDecision}\\
  \includegraphics[trim=0 10 20 20,clip,scale=0.6]{figs/fig2_hist_groups_rational}
};
\path(-5.5,4.7)node{\sf \textbf{A}}+(5.8,0)node{\sf \textbf{B}}+(0.6,-5.3)node{\sf \textbf{C}};
\end{tikzpicture}
\caption{{Tax rates implemented in groups.} Histograms for the TAX (A) and the MIN (B) frame with 48 groups each. (C) shows rational egoistic preferences of the median members for the 48 groups. }\label{fig:hist_decision}
\end{figure}

 Figures \ref{fig:hist_ideal} and \ref{fig:hist_decision} show the histograms of individual ideally preferred levels of redistribution before deliberation and the histograms of the collective decisions. 
 Decisions about a minimal income $m$ were displayed as the corresponding tax rate $\tau$ for which subjects decided implicitly. Both figures also include a histogram of the rational egoistic preferences. 

Ideally prefered tax rates as well as group decisions are substantially and significantly higher in the MIN frame than in the tax frame. 
Framing is a vehicle to manipulate the mental representations of reality (e.g. general redistribution vs. basic security). Framing thereby triggers the prevalence of different normative views on how to behave in certain situations (e.g. distributional justice vs. need based justice). 

The two mechanisms MIN and TAX are experimental vehicles that do not have one-to-one equivalents in real-world politics. There are no systems in which the minimal income is fully determined by a single redistributive tax rate, are there systems where the revenue of a certain tax is directly transferred back to tax payers as  equal lump-sums. Nevertheless, there is no doubt that both parameters, the income of the poorest and the level of taxation, are constantly at the top of the political agenda and heavily discussed outcomes of policy decisions. In real world politics the causal link between the income of the poorest and how it is achieved through spending taxpayers' money is certainly less comprehensible to voters than in our laboratory democracy. In this respect it is even more compelling to see that even if a simple direct relation between taxation and benefits exists, framing provides an effective tool to alter individual and collective decisions on redistributive taxation. 

From a normative perspective, the framing effect can be regarded as a problem, because equal societal choices on redistribution are steered in fundamentally different directions only because of a different way of presenting these choices. In real elections voters typically decide on more abstract ideological positions of parties (left vs.~right), where the desired level of redistribution is typically seen as a very important determinant of the ideological position. In this perspective our results confirm that it might be crucial for election results which policy frame is more salient in public debate.
Changing the frame in which the issue of redistribution is deliberated provides leverage to manipulate the final outcome of groups' redistributive decision. This may explain why political debates of social versus liberal economic policies often center on drawing the discourse to different policy frames; with liberals pronouncing the tax frame and socialists pronouncing the minimal income frame.


\section{Systemic Growth Effects}

 Two mainstream beliefs are that easy options for the imitation of technology as well as excessive redistribution both hinder societal growth. 
 The argument in both cases is that disincentives for the individual are created.  
 Immitation stops firms from doing innovation. 
 Redistribution hinders the wealthy ones to achieve their maximal growth potential, which is obviously larger in expectation when growth is multiplicative. 
 The argument is that these individual effects add up and consequentyl hinder also societal growth. 
 From a systemic perspective instead it becomes clear how imitation and redistribution also spur the growth of the system. 

 In the model of imitation and innovation firms decide strategically if they opt for the imitation or the innovation strategy. 
 It turns out that the growth rate of productivity is higher compared to an economy of firms which all use the innovation strategy. 
 Imitation fulfills a pushing function. 
 Firms lacking behind in technology can catch up faster, and thus the base for firms taking steps forward at the technology frontier through innovation is larger. 
 Nevertheless, innovation remains the only process which can bring the technology frontier forward, but the ``pushing'' function of imitiation is overlooked by focussing on the individualistic perspective only. 
 
 In a society with multiplicative stochastic growth, wealthy individuals can indeed grow more in absolute terms than poor individuals. 
 Let us consider an example of two individuals starting with a wealth of one each. 
 Let us further consider that there are ten rounds and in each round one of the individuals doubles its wealth while the other one halves it. 
 If all doubling events happen to the same individual the total wealth after round ten is very large $2^10 + 0.5^10 = 1024.001$. 
 This is the largest possible outcome, but much more likely is that both individual have five successess and five failures. 
 In this case the total wealth after round ten is the same as in round zero $2^5\cdot 0.5^5 + 2^5\cdot 0.5^5 = 2$. 
 When individuals would fully redistribute after every round they would end up with a total wealth of $(\frac{2 + 0.5}{2})^10 + (\frac{2 + 0.5}{2})^10 = 18.63$ for any distribution of successes among the individuals.
 So, in a very rare case the total wealth is much higher without redistribution, but for the typical case redistribution performs much better. 
 
 The probability of all individuals having all successes is $\Sexpr{round(dbinom(10, 10, 0.5),digits=4)}$, while the probability that both have five successes is $\Sexpr{round(dbinom(5, 10, 0.5),digits=3)}$. 
 This imbalance increases quickly with more time steps such that the situations where no redistribution is beneficial are practically not happening. 
 It seems that humans have problems to recognize and appreciate this effect, probably because humans typically assign the reason for their success and thesuccess of others to ability while only failures are assigned to bad luck. 
 Most popular arguments in favor of redistribution are the prevention of unrest, a democratic process where poor people have a majority, or moral arguments of charity or justice as a reason. 
 Perhaps these moral emotions or societal institutions evolved in human societies because they spur the growth of these societies through the mechanism of portfolio rebalancing presented here. 
 
 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% CHAPTER 4 %%%%%%%%%%%%%%%%%
\chapter{The Wisdom of Crowds}\label{ch:woc}
\mbox{}\\[-1cm]
{\footnotesize
Papers of the Habilitation Work: \\[-1.5mm]
\begin{compactenum}
\setcounter{enumi}{9}
\item \fullcite{Rauhut.Lorenz2010wisdomofcrowds}
\item \fullcite{Lorenz.Rauhut.ea2011HowSocialInfluence} \\
Reply Letter: \fullcite{Rauhut.Lorenz.ea2011ReplytoFarrell}
\item \fullcite{Lorenz.Rauhut.ea2015MajoritarianDemocracyUndermines}
\end{compactenum}
}
\vspace{1cm}


\hfill\parbox{100mm}{
\footnotesize
In these democratic days, any investigation into the trutstworthiness and peculiarities of popular judgments is of interest. % [...] This result is, I think more credible to the trustworthiness of a democratic judgment than might have been expected. 
\hfill Francis Galton (\citeyear{Galton1907Voxpopuli})
}

\medskip

 The wisdom of the crowd phenomenon was popularized by \textcite{Surowiecki2004wisdomofcrowds}. 
 Nowadays, the term ``crowd wisdom'' is used as an explanation for the functioning of a variety of phenomena like crowd sourcing and crowd funding or even democracy. 
 A typical metaphor is that humans can copy the concept of swarm intelligence from social animals. 
 Social insects are for example able to solve complex problems such as the best spot for a new nest without any central organization. 
 In this chapter we look at one particularily simple instance of crowd wisdom: \emph{guesstimation tasks}. A guesstimate is an estimate made without complete information but by guesswork or conjecture. 
 Guesstimation is the main method for many predicition tasks.
 Thus it is very relevant for a lot of individual or societal decision making. 
 In particular, we will only deal with guesstimation tasks for numerical facts. 
 In that sense, the disussion and change of guesstimates about numerical facts, is a form of opinion dynamics close to the models presented in Chapter \ref{ch:od}.

 The seminal example is due to a paper of one founding father of statistics -- Francis Galton. 
 At the weight-judging competition at the West of England Fat Stock and Poultry Exhibition he collected 787 cards on which competitors estimated the weight of the meat of an ox after it has been slaughtered and dressed. 
 In his paper in \emph{Nature}, \textcite{Galton1907Voxpopuli} reported that the 5-percentile and the 95-percentile spannded values from 1,074 to 1,293 pounds, while the median (middlemost) value was 1,207 which was surprisingly close to the correct value of 1,198 pounds. 
 \textcite{Galton1907Voxpopuli} concluded: 
\emph{``This result is, I think more credible to the trustworthiness of a democratic judgment than might have been expected.''}

 Following Galton, we speak of the wisdom of the crowd phenomenon in numerical guesstimation tasks when aggregate results are ``more credible than expected'', i.e. when the aggregate of several individual estimates is close to the true value although most estimates are far away from it. 
 Thus, crowd wisdom is something which goes beyond individual wisdom on the same subject.
 The part ``beyond individual wisdom'' is often a source of confusion in assessing crowd wisdom. 
 The terminology used here takes this part serious. 
 We would not call a crowd ``wise'', when every individual is as ``wise'' as the crowd. 
 Put the other way round, crowd wisdom relies on limitations of its individuals. 
 
 In this chapter, we will often take the perspective of an external observer who wants to extract the crowd's wisdom from diverse responses by appropriate mathematical tools and also appropriate rules of exchange and collective judgment for the crowd. 
 


\section{Three guesstimation games} \label{sec:guesstimation}

 Just recently, Kenneth W. Wallis dug out the full sample of estimates from Galton's notes out of the archive and made it publically available\footnote{\url{https://www2.warwick.ac.uk/fac/soc/economics/staff/academic/wallis/publications/galton_data.xlsx}}. He also found some small errors in Galton's computation \parencite{Wallis2014RevisitingFrancisGaltons} with a 5-percentile of \Sexpr{format(quantile(G,0.05),digits=4,big.mark=",")} and a median of \Sexpr{format(median(G),big.mark=",")}. Figure \ref{fig:galton} shows the histogram of estimates together with the truth, the median and the mean of estimates. 
 
 Interestingly, the mean of estimates \Sexpr{format(mean(G), digits=5,big.mark=",")} is even closer to the truth than the median  which Galton reported. \textcite{Hooker1907MeanorMedian} already made this point in a letter to \emph{Nature}. He estimated the mean as 1,196 based on the percentiles reported by \textcite{Galton1907Voxpopuli}. In a reply \textcite{Galton1907Ballot-Box} reported it to be 1,197. Hooker as well as Galton referred to another paper, where \textcite{Galton1907Onevoteone} argued that for ``democratic'' aggregation always the median shall be used and not the mean, because it would give \emph{``voting power to `cranks' in proportion to their crankiness.''} Ultimately, it turns out that the arguments for using the median as democratic means against cranks has its value in its own right, although it is not of key importance in guesstimation tasks. The malicious power of cranks under aggregation of the mean would be to over- or understate estimates to steer the mean towards a certain direction. 
 Obviously, this does not play such an important role in guesstimation games, first because there is no incentive to do, and second because the number of participants is large. 
 
\begin{figure}
<<Galton-Plot,echo=FALSE, fig.height=3.5>>=
ggplot(tbl_df(G), aes(value)) + geom_histogram(binwidth = 5) + 
  scale_x_continuous(breaks=seq(900,1500,by=100)) + theme_bw() + labs(x="Estimates: weight ox' meat (pounds)") +
  annotate("pointrange", x=median(G), y = 45, ymin = 0, ymax = 45, colour = "red", size = 1) +
  annotate("text", label="Median", x=median(G)+15, y = 45, colour = "red", hjust=0) +
  annotate("pointrange", x=mean(G), y = 55, ymin = 0, ymax = 55, colour = "blue", size = 1)+
  annotate("text", label="Mean", x=mean(G)+15, y = 55, colour = "blue", hjust=0) +
  annotate("pointrange", x=mean(truth_G), y = 50, ymin = 0, ymax = 50, colour = "darkviolet", size = 1) +
  annotate("text", label="Truth", x=truth_G+15, y = 50, colour = "darkviolet", hjust=0) 
@
\caption{Histogram of estimates for the weight-judging competition at the West of England Fat Stock and Poultry Exhibition in Plymouth 1907 ($N=\Sexpr{length(G)}$). }\label{fig:galton}
\end{figure}

 The question of mean vs. median, or -- more general -- the choice of the aggregation function can be elaborated further with another impressive example for the wisdom of the crowd phenomenon. This time it involves data in which collection I was involved. 
 The lottery ``Haste mal 'nen Euro? -- Tombola''\footnote{Meaning something like ``Would you spend one Euro? -- Tombola''.} happened at the festival ``ViertelFest'' August 22-24, 2008 in Bremen. The tombola was an ordinary one, where people could buy lots which could bring them prices. As the topic of the festival was ``Swarm Intelligence'', I asked the organizers to include a guesstimation task similar to the one of the West of England Fat Stock and Poultry Exhibition. 
 Each buyer of a lot also got another chance to win: 
 They could estimate the number of lots which would have been sold in total after the three days of the festival. The estimates closest to the true value got tickets for the circus. Further on, before the lottery the organizers asked an expert, the organizer of Bremen's biggest lottery (B체rgerpark-Tombola) about his estimate for reference. The data is accessible online\footnote{Blog post in German: \url{http://janlo.de/wp/2010/06/22/die-weisheit-der-bremer/}. Direct link to the data: \url{http://spreadsheets.google.com/pub?key=0AqRUj58gkTakcDIybDYtYTRBOHozTDVtRkhCX1N4Rnc&hl=de&single=true&gid=0&output=csv}}. 

 In total, \Sexpr{format(length(sD), big.mark=",")} estimates of the number of lots were handed in and \Sexpr{format(truth, big.mark=",")} lots had been sold\footnote{Thus, \Sexpr{format(100*length(sD)/truth, digits=3)}\% of the lots were used to participate in the guesstimation game.}. The expert estimated \Sexpr{format(expert, big.mark=",")} lots. 
 The 5- and 95-percentiles spanned values from \Sexpr{format(round(quantile(sD,0.05)), big.mark=",")} to \Sexpr{format(round(quantile(sD,0.95)), big.mark=",")}, and thus almost three orders of magnitude. 
 The median of estimates was \Sexpr{format(median(sD),big.mark=",")} and came much closer to the true value than the expert. 
 Figure \ref{fig:viertelfest} shows the truth, the experts estimate, the mean\footnote{The arithmetic mean is meant here. }, the median and the histogram of estimates with the x-axis clipped such that all estimates larger than 150,000 are not visible. 
 There were \Sexpr{format(sum(sD>150000),big.mark=",")} estimates larger than 150,000 (\Sexpr{format(100*sum(sD>150000)/length(sD),digits=2)}\%) ranging from \Sexpr{format(min(sD[sD>150000]),big.mark=",")} to \Sexpr{format(max(sD[sD>150000]),big.mark=",")}.
 These large estimates (especially the maximal one) make the mean such a bad aggregation function in this case.  Nevertheless, even when these \Sexpr{format(sum(sD>150000),big.mark=",")} values were removed the mean would be \Sexpr{format(round(mean(sD[sD<=150000])),big.mark=",")} and still way too high. 

\begin{figure}
<<Viertelfest-Hist, echo=FALSE, fig.height=3.5>>=
Dfit <- fitdist(sD,"lnorm")
br <- seq(0,150000,by=1000)
ggplot(filter(tbl_df(sD),value<150000),aes(value)) + geom_histogram(binwidth = 1000) + 
  scale_x_continuous(breaks=seq(0,150000, by=20000)) + theme_bw() +
  labs(x="Estimates: number of lots") +
  annotate("pointrange", x=expert, y = 30, ymin = 0, ymax = 30, colour = "darkgreen", size = 1) +
  annotate("text", label="Expert", x=expert+3000, y = 30, colour = "darkgreen", hjust=0) +
  annotate("pointrange", x=median(sD), y = 100, ymin = 0, ymax = 100, colour = "red", size = 1) +
  annotate("text", label="Median", x=median(sD)+3000, y = 100, colour = "red", hjust=0) +
  annotate("pointrange", x=exp(mean(log(sD))), y = 90, ymin = 0, ymax = 90, colour = "brown", size = 1)+
  annotate("text", label="Geometric Mean", x=exp(mean(log(sD)))+3000, y = 90, 
           colour = "brown", hjust=0) +
  annotate("pointrange", x=mean(sD), y = 10, ymin = 0, ymax = 10, colour = "blue", size = 1)+
  annotate("text", label="Arithmetic Mean", x=mean(sD)+3000, y = 10, colour = "blue", hjust=0) +
  annotate("pointrange", x=mean(truth), y = 80, ymin = 0, ymax = 80, colour = "darkviolet", size = 1) +
  annotate("text", label="Truth", x=truth+3000, y = 80, colour = "darkviolet", hjust=0) 
@
\caption{Histogram of estimates for the guesstimation task ``How many lots will be sold at the end of the festival'' at the Viertelfest Aug 22-24, 2008 in Bremen. ($N=\Sexpr{format(length(sD), big.mark=",")}$, x-axis clipped at 150,000)}\label{fig:viertelfest}
\end{figure}

 Here, the median performs quite well, as in Galton's data. 
 The mean instead performs bad.
 A reason is that the distribution is heavily right-skew with a fat right tail.  
 This is typical for distributions when numbers are bounded by zero but span several orders of magnitude. 
 In this situation humans face a problem of logarithmic nature. 
 That means they firstly cope with finding the right magnitude. 
 Logarithmic thinking also seems to be the natural human intuition for mapping numbers to a line, while the linear mapping is a cultural invention needing formal education \parencite{Dehaene.Izard.ea2008LogorLinear}. 
 In these situations, typically the logarithm of estimates looks more like a normal distribution. 
 Thus, the log-normal distribution is a better fitting distribution and the geometric mean\footnote{The geometric mean is the exponential of the arithmetic mean of the logarithmized sample data. } is a better measure of central tendency than the arithmetic mean. 
 The geometric mean is \Sexpr{format(exp(mean(log(sD))), digits=5,big.mark=",")} and thus only \Sexpr{format(truth-exp(mean(log(sD))), digits=3,big.mark=",")} less than the correct answer. 
 
 The former discussion gives rise to the claim that the shape of the distribution can advise the choice of the right measure of aggregation. Figure \ref{fig:WoCdistr} shows density estimates and best fits for a normal and a log-normal distribution for both samples. For Galton's data the normal distribution is the better fit according to all goodness-of-fit measures\footnote{All measures provided by \texttt{gofstat} in the R-package \texttt{fitdistr}.}. For the Viertelfest data instead the log-normal fit is better by all goodness-of-fit measures. 
 The median of a normally distributed random variable coincides with the arithmetic mean, while the median of a lognormally distributed random variable coincides with the geometric mean. This further underpins that the different choices of the arithmetic and the geometric mean for the two samples is quite reasonable. 
 
 \begin{figure}
<<WoC-Distr, echo=FALSE,fig.height=3>>=
LOGNfit <- fitdist(G,"lnorm")
NORMfit <- fitdist(G,"norm")
br <- seq(850,1550,by=1)
DISTS <- data_frame(x=br, ylogn=dlnorm(br,meanlog=LOGNfit$estimate[1],sdlog=LOGNfit$estimate[2]),
                    ynorm=dnorm(br,mean=NORMfit$estimate[1],sd=NORMfit$estimate[2]))
gG <- ggplot(tbl_df(G),aes(value)) + geom_density(bw=5) + 
  geom_line(data=DISTS,mapping=aes(x=x, y=ylogn), color="brown") +
  geom_line(data=DISTS,mapping=aes(x=x, y=ynorm), color="blue") +
  scale_x_continuous(breaks=seq(900,1500,by=100)) + theme_bw() + 
  labs(x="Estimates: weight of ox' meat (pounds)")
LOGNfit <- fitdist(sD,"lnorm")
NORMfit <- fitdist(sD,"norm")
br <- seq(0,150000,by=100)
DISTS <- data_frame(x=br, ylogn=dlnorm(br,meanlog=LOGNfit$estimate[1],sdlog=LOGNfit$estimate[2]),
                    ynorm=dnorm(br,mean=NORMfit$estimate[1],sd=NORMfit$estimate[2]))
gD <- ggplot(filter(tbl_df(sD),sD<150000),aes(value)) + geom_density(bw=1000) + 
  geom_line(data=DISTS,mapping=aes(x=x, y=ylogn), color="brown") +
  geom_line(data=DISTS,mapping=aes(x=x, y=ynorm), color="blue") +
  scale_x_continuous(breaks=seq(0,100000, by=50000)) + theme_bw() +
  labs(x="Estimates: number of lots", y="")
grid.newpage()
grid.draw(cbind(ggplotGrob(gG),ggplotGrob(gD), size="first"))
@
\caption{Density plots (black) and Maximum-Likelihood fits for normal (blue) and log-normal (brown) distributions of Galton's data (left, with density bandwidth 1,000) and the Viertelfest data (right, with density bandwidth 5). The x-axis for Viertelfest data is clipped at 150,000 and the normal distribution there looks almost flat because it has a very large variance. }\label{fig:WoCdistr}
\end{figure}

 Inference of the best measure of central tendency for the aggregation of the wisdom of the crowd from measurements of other aspects of the distribution is an interesting task which needs further theoretical and empirical exploration. 
 One way is to use the augmented quincunx model of sequential and probabilistic cue categorization presented by \textcite{Nash2014CuriousAnomalySkewed}. 
 In particular, he argues that left-skewness in Galton's sample is due to the fact that most judgers had a lot of professional knowledge and the fact that the ox was comparably heavy. 
 According to the theory, extremeness of the true value in comparison to a typical value known by judgers triggers a skew distribution, which is skew on the side of the typical and steep on the side of the extreme.  
 
 The set of options for measures of aggregtion is well described by \emph{abstract means} as outlined by \textcite{Hegselmann.Krause2005OpinionDynamicsDriven}\footnote{\textcite{Hegselmann.Krause2005OpinionDynamicsDriven} extend the concept to partial abstract means which they use to generalize bounded confidence models of opinion dynamics as treated in Chapter \ref{ch:od}. }. An abstract mean is a function which maps a set of real numbers to a certain number between the maximum and the minimum of that set.
 Within this framework it is always possible to find an abstract mean which aggregates the truth correctly as long as the truth is sandwiched between the minimal and the maximal estimate. 
 This excercise is of little practical relevance. 
 The real challenge is to infer a good generalized mean from the distribution of estimates without knowing the truth. 

\begin{figure}
<<OL-Plot,echo=FALSE, fig.height=3.5>>=
ggplot(filter(tbl_df(sO),value<=5000), aes(value)) + geom_histogram(binwidth = 50) + 
  scale_x_continuous(breaks=seq(0,5000,by=500)) + theme_bw() + 
  labs(x="Estimates: number of estimation tickets") +
  annotate("pointrange", x=median(sO,na.rm=T), y = 16, ymin = 0, ymax = 16, colour = "red", size = 1) +
  annotate("text", label="Median", x=median(sO,na.rm=T)+110, y = 16, colour = "red", hjust=0) +
  annotate("pointrange", x=mean(sO,na.rm=T), y = 7, ymin = 0, ymax = 7, colour = "blue", size = 1)+
  annotate("text", label="Mean", x=mean(sO,na.rm=T)+110, y = 7, colour = "blue", hjust=0) +
  annotate("pointrange", x=mean(truth_O), y = 18, ymin = 0, ymax = 18, colour = "darkviolet", size = 1) +
  annotate("text", label="Truth", x=truth_O+110, y = 18, colour = "darkviolet", hjust=0) +
  annotate("pointrange", x=exp(mean(log(sO),na.rm=T)), y = 15, ymin = 0, ymax = 15, 
           colour = "brown", size = 1) +
  annotate("text", label="Geometric Mean", x=exp(mean(log(sO),na.rm=T))+110, y = 15, 
           colour = "brown", hjust=0) +
  annotate("pointrange", x=powermean(sO,-1), y = 17, ymin = 0, ymax = 17, colour = "darkcyan", size = 1) +
  annotate("text", label="Harmonic Mean", x=powermean(sO,-1)+110, y = 17, colour = "darkcyan", hjust=0) 
@
\caption{Histogram of estimates for the number of estimation tickets at the Lange Nacht der Wissenschaft at Carl von Ossietzky Universit채t Oldenburg 2010 ($N=\Sexpr{length(sO)}$, x-axis clipped at 5,000). }\label{fig:OL}
\end{figure}

 An example for a sample of estimates which shows little crowd wisdom is another dataset collected by me on another occasion. At the ``Lange Nacht der Wissenschaft''\footnote{Literally ``Long Night of Science''} at Carl von Ossietzky Universit채t Oldenburg on September 24, 2010, I collected, with the help of others, estimates in a guesstimation game\footnote{See a report in German here \url{http://janlo.de/wp/2010/09/07/schatzquiz-oldenburg/}. Link to the data: \url{https://docs.google.com/spreadsheets/d/1Bqe4yeb26lKIoaCqYfq4ZaK5jefSHGSRCpHlyWOiMQ4/edit?usp=sharing}} similar to the one of the Viertel-Fest. 
 Visitors were asked how many estimation tickets will be handed in between 17:00 and 20:00. 

 I collected \Sexpr{truth_O} tickets, that was the correct answer\footnote{For some reason, I reported 271 as the correct answer directly after the game. The reason is probably that I forgot to ultimately update the computation after entering the data from the final ticket. There was time pressure, because the winner had to be announced shortly after in the context of a science slam talk.}. From the tickets, I extracted \Sexpr{length(sO)} valid estimates which 5- to 95 percentile spanned values from \Sexpr{format(round(quantile(sO,0.05,na.rm=T)), big.mark=",")} to \Sexpr{format(round(quantile(sO,0.95,na.rm=T)), big.mark=",")}, and thus also almost three orders of magnitude. 
 The mean of estimates is \Sexpr{format(mean(sO,na.rm=T),digits=5,big.mark=",")}, the geometric mean is \Sexpr{format(exp(mean(log(sO),na.rm=T)),digits=4,big.mark=",")}, and the median is \Sexpr{format(median(sO,na.rm=T),big.mark=",")}. All are way to high compared to the true value. 
 Taking the median's closeness to the truth as a good proxy for crowd wisdom, we do not find much in this dataset. 
 The skewness and dispersion of the estimates over orders of magnitude is similar to the estimates from the Viertelfest. Thus, there is little indication coming from the data to use another aggregation function than the geometric mean, and it is reasonable to attribute this deviation to a systematic error caused by a cognitive bias to overestimate. 

 Nevertheless, there is a speculative interpretation of the estimation task which could give rise to another measure of aggregation which performs better. On the tickets, the time span 17:00-20:00 was clearly indicated. Based on this, we may interprete that people estimated how much time the collection takes per ticket. If we assume that people saw my three helpers and me, they might have thought about four times 180 minutes summing up to 720 collector minutes. The collection time per ticket was such 720/272 = 2.65 minutes. Estimates of 1000, or 5000 by other people would thus transform 720/1000=0.72 and 720/5000=0.14 minutes. With the assumption of this mindset, the arithmetic mean of these ratios might elicit some crowd wisdom, and the reciprocal value of this times 720 minutes would by the aggregate estimate of the crowd. This description coincides with taking the harmonic mean. The harmonic mean is the reciprocal value of the arithmetic mean of the reciprocal values of the sample data. The harmonic, the geometric and the arithmetic mean are special cases of $p$-means (also called power means or generalized means).\footnote{For positive numbers $x_1,\dots,x_n$ and a real number $p$, the $p$-mean is $(\frac{1}{n}\sum x_i^p)^\frac{1}{p}$. The arithmetic mean is represented by $p=1$, the harmonic mean by $p=-1$. The geometric mean appears as the limit for $p\to 0$. The minimum and the maximum are the limits for $p\to -\infty$ and $p\to\infty$ respectively. $p$-means of the same sample are monotonously decreasing with $p$.}. The harmonic mean of the estimates of the number of tickets is \Sexpr{format(powermean(sO,-1),digits=4,big.mark=",")} and comes close to the true value of \Sexpr{truth_O}. As a side note, this interpretation would in principle also be valid for the number of lots at the Viertelfest, but the timespan for collection and number of collectors were not prominently communicated to participants. 
 As there is little indication from the data to use the harmonic mean and the median is also far away from the truth, it is more reasonable to assume that a systematic error is the reason. 
 
 Figure \ref{fig:OL} shows the histogram of estimates with the x-axis clipped such that all estimates larger than 5,000 are not visible. 
 There were \Sexpr{format(sum(sO>5000,na.rm=T),big.mark=",")} estimates larger than 5,000 (\Sexpr{format(100*sum(sO>5000,na.rm=T)/length(sO),digits=2)}\%) ranging from \Sexpr{format(min(sO[sO>5000],na.rm=T),big.mark=",")} to \Sexpr{format(max(sO[sO>5000],na.rm=T),big.mark=",")}.

\begin{table}
\small
\begin{tabular}{>{\raggedleft\arraybackslash}m{0.22\textwidth}>{\raggedleft\arraybackslash}m{0.22\textwidth}>{\raggedleft\arraybackslash}m{0.22\textwidth}>{\raggedleft\arraybackslash}m{0.22\textwidth}} \toprule 
 & Galton & Viertelfest & Oldenburg \\ \midrule\midrule
Collection date & 1906 or 1907 & 2008-08-22 to 24 & 2010-09-24 17:00-20:00 \\ \addlinespace[0.5ex]
Place & West of England Fat Stock and Poultry Exhibition, Plymouth, UK & Viertelfest Bremen, DE & Lange Nacht der Wissenschaft, Carl von Ossietzky Universit채t Oldenburg, DE \\ \addlinespace[0.5ex]
Collected by & Passed to Francis Galton by organizers & Volunteers from the lottery, passed to Jan Lorenz & Jan Lorenz with volunteers \\ \addlinespace[0.5ex]
Number of estimates $N$ & \Sexpr{length(G)} & \Sexpr{format(length(sD),big.mark=",")} & \Sexpr{length(sO)} \\ \addlinespace[0.5ex]
guesstimation task & Weight of ox's meat after slaughtered and dressed & Number of lots sold at festival & Number of estimation tickets handed in \\ \addlinespace[0.5ex]
True value & \cellcolor{violet!20}\Sexpr{format(truth_G,big.mark=",")} & \cellcolor{violet!20}\Sexpr{format(truth,big.mark=",")} & \cellcolor{violet!20}\Sexpr{format(truth_O,big.mark=",")} \\ \addlinespace[0.5ex]
Minimum & \cellcolor{red!20}\Sexpr{format(min(G),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(min(sD),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(min(sO),big.mark=",")} \\ \addlinespace[0.5ex]
5-percentile & \cellcolor{red!20}\Sexpr{format(round(quantile(G,0.05)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(quantile(sD,0.05)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(quantile(sO,0.05)),big.mark=",")} \\ \addlinespace[0.5ex]
Harmonic mean & \cellcolor{yellow!20}\Sexpr{format(round(powermean(G,-1)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(powermean(sD,-1)),big.mark=",")} & \cellcolor{green!20}\Sexpr{format(round(powermean(sO,-1)),big.mark=",")} \\ \addlinespace[0.5ex]
Median & \cellcolor{yellow!20}\Sexpr{format(round(median(G)),big.mark=",")} & \cellcolor{yellow!20}\Sexpr{format(round(median(sD)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(median(sO)),big.mark=",")} \\ \addlinespace[0.5ex]
Geometric mean & \cellcolor{yellow!20}\Sexpr{format(round(powermean(G,0)),big.mark=",")} & \cellcolor{green!20}\Sexpr{format(round(powermean(sD,0)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(powermean(sO,0)),big.mark=",")} \\ \addlinespace[0.5ex]
Arithmetic mean & \cellcolor{green!20}\Sexpr{format(round(powermean(G,1)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(powermean(sD,1)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(powermean(sO,1)),big.mark=",")} \\ \addlinespace[0.5ex]
95-percentile & \cellcolor{red!20}\Sexpr{format(round(quantile(G,0.95)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(quantile(sD,0.95)),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(round(quantile(sO,0.95)),big.mark=",")} \\ \addlinespace[0.5ex]
Maximum & \cellcolor{red!20}\Sexpr{format(max(G),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(max(sD),big.mark=",")} & \cellcolor{red!20}\Sexpr{format(max(sO),big.mark=",")} \\ \addlinespace[0.5ex]
Deviation best mean from truth & -\Sexpr{format(100*abs(powermean(G,1)-truth_G)/truth_G,digits=3)}\% & -\Sexpr{format(100*abs(powermean(sD,0)-truth)/truth,digits=3)}\% & +\Sexpr{format(100*(powermean(sO,-1)-truth_O)/truth_O,digits=3)}\% \\ \addlinespace[0.5ex]
Deviation median from truth & +\Sexpr{format(100*(median(G)-truth_G)/truth_G,digits=3)}\% & -\Sexpr{format(100*abs(median(sD)-truth)/truth,digits=3)}\% & +\Sexpr{format(100*(median(sO)-truth_O)/truth_O,digits=3)}\% \\ \addlinespace[0.5ex]
 \bottomrule
\end{tabular}
\caption{Overview about the three example datasets on the wisdom of the crowd. For better alignment most values have been rounded to full integers. Color code: violet = true values, red = far away from true value, yellow = close to true value, green = closest to true value.} \label{tab:WoC}
\end{table}

 Table \ref{tab:WoC} summarizes information and measures for the three samples from Galton, the Viertelfest, and Oldenburg. 
 The table concludes with measures reporting the number of estimates which were closer to the true value than the wisdom of crowd measure. The second last row shows how many people were better than the best aggregating mean (the arithmetic, geometric, and harmonic for Galton, Viertelfest, and Oldenburg respectively). The last row shows how many people were better than the median, because this is the measure which performs best over all three datasets. 
 
 
 \section{On quantifying crowd wisdom} \label{sec:quantwoc}
 
 We defined crowd wisdom as wisdom going beyond individual wisdom. Not all samples of estimates for a particular guesstimation task can elicit crowd wisdom. Let us distinguish two extreme cases which can serve to assess when crowds are wise and when not. 
\begin{compactenum}[(i)]
 \item When all individuals estimate correctly, then the crowd is not wiser than the individual. Thus, there is no crowd wisdom which goes beyond individual wisdom. More general statement: If the sample's central tendancy is close to the truth but estimates show little dispersion, then there is not much crowd wisdom. 
 \item When the truth is not sandwiched between the minimal and maximal estimate, then there is no wisdom in the crowd but a systematic error in the sample. More general statement: When the reasonable measure of central tendency of the sample is far away from the truth, then there is no crowd wisdom. 
\end{compactenum}

 Let us rephrase this with the notions of the International Organization for Standardization (ISO) described in ISO 5725 (six parts) ``Accuracy (trueness and precision) of measurement methods and results''. 
 We see estimates drawn from a crowd as a measurement method for a particular fact, here the true value. According to ISO 5725 \emph{accuracy} of a measurement method is characterized by trueness and precision. 
 \emph{Trueness} is the closeness of agreement between the arithmetic mean of a large number of measurements and the true or accepted reference value. 
 \emph{Precision} instead is the closeness of agreement between different measurements. 
 According to this terminology, the wisdom-of-crowd phenomenon appears when estimates from a crowd show high trueness, but low precision. 

 In terms of statistics the measure of aggregation of estimates is a \emph{statistical estimator of the true value}. 
 The estimating crowd shows crowd wisdom when the estimator has a very small statistical bias, but very high variance and mean squared error. 
 In terms of measurement error, the statistical bias is caused by \emph{systematic error} while variance (analogously low precision) is caused by \emph{random error}. 
 The term statistical bias is purely statistical and should not be confused with systemic or cognitive bias which relate bias to institutional or mental processes which cause the statistical bias. 
 
 A fundamental mathematical relation of the mean squared error, the variance and the statistical bias is the \emph{bias-variance decompostion of squared error} which states that the mean of squared errors is equal to the squared bias plus the variance of estimates. More suggestive of the wisdom of crowds, \textcite{Page2007DifferenceHowPower} calls it the \emph{diversity prediction theorem}, which states
 \begin{quote} 
 Collective Error = Average Individual Error -- Diversity,
 \end{quote}
 where \emph{collective error} is squared bias, \emph{average individual error} is the mean of squared errors, and \emph{diversity} is the variance.  
 This theorem implies that higher diversity decreases the collective error. This message is a bit suggestive as it blurs over the fact that increased variance (diversity) naturally also implies an increase of the average individual error. 
 For example if we add a number from a normal distribution to each estimate in a sample we increase both, the diversity but also the average individual error, while the collective error remains the same. 
 A less suggestive message to extract from the theorem is that in case of a large average individual error, large diversity is needed for small collective error. 
 
 All the notions mentioned above are listed in Table \ref{tab:WoCNotions}. These notions apply to the arithmetic mean as the measure of aggregation for the crowd's estimates. In line with \textcite{Page2007DifferenceHowPower}, we use the squared distance of the aggregate to the true value as the definition for the collective error. The square makes some mathematics easier but is not a standard definition. Instead, the definitions of variance and mean squared error are standard.

\begin{table}[htbp]
\footnotesize
\def\tabularxcolumn#1{m{#1}}
\begin{tabularx}{\textwidth}{@{}m{24mm}  >{\centering\arraybackslash}m{30mm} >{\centering\arraybackslash}X >{\centering\arraybackslash}m{27mm} @{}} \toprule
\textbf{Predicting Crowd} \newline \parencite{Page2007DifferenceHowPower}        & as $\{x_1,\dots,x_N\}$ \newline with truth $y$ & as a \newline Statistical Estimator  & as an ISO 5752 \newline Measurment Method  \\ \midrule
\textbf{Collective Error =}  &    $(y - \frac{1}{N}\sum\limits_{i=1}^N x_i)^2$ & (squared) \newline Statistical Bias$\mbox{}^\ast$ \newline (by systematic error) &  (an inverse of)  \newline Trueness \\ \midrule
\textbf{Average \newline Individual Error =}    & $\frac{1}{N}\sum\limits_{i=1}^N(y - x_i)^2$ &  Mean Squared Error & \\ \midrule 
\textbf{Diversity =}  & $\frac{1}{N}\sum\limits_{i=1}^N(x_i - \frac{1}{N}\sum\limits_{i=1}^Nx_i)^2$ & Variance \newline (by random error)  & (an inverse of) \newline Precision \\ 
\midrule
& \multicolumn{3}{l}{\scriptsize $\mbox{}^\ast$ called \emph{Population Bias} by \textcite{Vul.Pashler2008MeasuringCrowdWithin}} 
\end{tabularx}
\caption{Notions to quantify the wisdom of crowds in guesstimation games based on the notions of the diversity prediction theorem of \textcite{Page2007DifferenceHowPower}. }\label{tab:WoCNotions}
\end{table}




 \section{Repeated guesstimation}
 
 What happens to crowd wisdom when people participate more than once in the same guesstimation game and moreover exchange information about the estimates of others? This general question was analyzed in a controlled group estimation experiment where subjects sat in cubicles in front of computers. The experiment is documented in the supplemtary material of \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence}. 
 
 Subjects had to estimate facts. For each estimate, subjects could earn money based on the closeness of their individual estimate to the true value. Each subject had to enter five consecutive estimates for each question. Three different treatments for the exchange of estimates among subjects were used. 
 \begin{compactenum}[(i)]
\item \textbf{No information:} Subjects got no information about the estimates of other subjects. Thus, they estimated five times in a row without further information. This treatment served on the one hand as a control treatment for the other two treatments, and on the other hand it was used to assess what \textcite{Vul.Pashler2008MeasuringCrowdWithin} call the ``crowd within''. 
\item \textbf{Aggregated information:} After each round of individual estimates subjects were informed about the arithmetic mean of all estimates. Thus, subjects were confronted with a crowd estimate but not with the crowd's diversity. 
\item \textbf{Full information:} Subjects got information about the estimates of all other subjects, including a graphical visualisation. Here, subjects could observe the diversity of estimates among the crowd as well as changes of estimates of individuals over time. 
\end{compactenum}
Further on, in each treatment and for each question subjects were asked about their subjective certainty about their first and about their fifth and last estimate. 

\paragraph{On the wisdom of the crowd within.} 
 \textcite{Vul.Pashler2008MeasuringCrowdWithin} showed that humans can simulate a diverse crowd in their brain by repeated estimation. 
 They gave people guesstimation tasks and after their estimates they asked them to estimate again. They also had a treatment, where subjects were asked again three weeks later, which made the effect stronger. 
 They showed that the mean squared error of the average of these two guesses is lower than that of the first as well as the second guess. 
 From that they concluded that, knowledge has a probabilistic representation in the brain, because if the first guess is a deterministic best guess, then a second guess would only add noise with no decreasing effect on the mean squared error. 
 They also computed that one would on average need to ask 1.11 (1.32 in the delayed treatment) different people to achieve the same decrease of mean squared error than asking the same person twice. 

 \textcite{Rauhut.Lorenz2010wisdomofcrowds} used the ``no information'' condition of the experiment to elicit how the result of \textcite{Vul.Pashler2008MeasuringCrowdWithin} extends to asking oneself more than two times. 
 The analysis was embedded in the framework of the diversity predicition theorem and a method to compute the \emph{average number of different individuals one needs to ask to achieve the same improvement than asking oneself $T$ times} was developed. 
 The method coincides with the concept of \textcite{Vul.Pashler2008MeasuringCrowdWithin} for two consecutive estimates, but deviates for more estimates. 
 The method can also be used to quantify the average number of different individuals one needs to ask to achieve the same improvement than asking oneself ad infinitum.
 
 Asking oneself ad infinitum did on average not outperform asking only one other person. Over all our six questions the effect and also the magnitude 1.1 for asking oneself twice could be confirmed. Asking oneself ad infinitum (without a long delay) was on avarage as good as asking 1.28 different people. 
 Further on, there was variation regarding the guesstimation task. 
 For some of our questions there was no improvement through asking oneself twice or often. For one question subjects we worsened. 
 Thus, the hypothesis that people sample from a ``mental'' distribution when they estimate several times is questionable in these cases. 
 For other questions instead, asking oneself ad infinitum had the same benefit as asking up to 2.7 different people. 
 Finally, the formal measurement framework for the size of the crowd within of \textcite{Vul.Pashler2008MeasuringCrowdWithin} was extended to more than one estimate. Their idea turns out to overestimates the effect dramatically. 
 
 An open question remains the characterization of the type of guesstimation tasks for which improvements through repeatedly asking oneself are beneficial and for which tasks it is not. 
 Another open question is to what extend our incentives impact the results. 
 As each estimate triggered a payment based on closeness to the truth, some subjects probably placed their five different estimates attempting to cover all potential areas where they could imagine the true value. This behavior would increase the variance of estimates from one individual. 
 
\paragraph{On social influence and the wisdom of the crowd.}
 \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} found that the exchange of information substantially lowers the diversity of estimates while the collective error decreases only very mildly. The decrease of diversity from initial to later rounds of estimates was statistically significant, the decrease of collective error was not. In the ``no information'' treatment there was no significant change of diversity and collective error from initial to later estimates. 
 The information exchange in the experiment can be interpreted as a very mild form of social influence, because there were no options and no incentives to persuade others. 
 The first empirical finding is thus, that even this mild form of social influence decreases diversity without substantially decreasing collective error. 
 In terms of using the crowd as a measurement device, we increase precision, without increasing trueness, and as a wise crowd is a crowd with high trueness and low precision, social influence undermines the wisdom of crowd effect. 
 
 \textcite{Farrell2011Socialinfluencebenefits} pointed out that also the mean squared error decreases and as a consequence average payoff of individuals rises. 
 The result is thus, not that individuals suffer from social influence, but that the amount of crowd wisdom which goes beyond individual wisdom can be overestimated when the crowd exchanged information beforehand through what one might call a collective tunnel vision \parencite{Rauhut.Lorenz.ea2011ReplytoFarrell}. 
 
 Further on, average certainty increases under social influence, while it does not increase without. 

 
 \section{Wisdom-of-crowd indicator} \label{sec:wocindicator}

 A quantification of crowd wisdom must simultaneously quantify low collective error and high diversity to a degree such that the accuracy of the aggregated groups estimates is ``surprisingly'' good. For example, Galton's sample shows a lower collective error then the Viertelfest sample, but the Viertelfest data shows higher diversity. 
 The answer to the question, which of the two datasets shows more crowd wisdom, depends thus on our assessment of the degree of ``surprisingness''. 
 
 One way to measure this is to take the absolute value of the standard score (z-score) of the truth. 
 This is the distance of the mean to the truth divided by the standard deviation
 This measure depends on aggregation of a collective estimate by the arithmetic mean. 
 In Galton's sample the standard score of the truth is \Sexpr{format(standardScoreTruth(G,truth_G),digits=3)}, meaning that the distance of the arithmetic mean to the truth is just \Sexpr{format(standardScoreTruth(G,truth_G),digits=3)} standard deviations. Unfortunatelly, generalizations of this method to other aggregation functions like the geometric or the harmonic means rely on transformations of the data and an appropriate definition of the z-score, including a the definition of something like a geometric or harmonic standard deviation, which altogether seems not straightforward and consequently difficult to compare\footnote{The most straightforward way to quantify adjuted standard scores when the geometric (respectively the harmonic) mean is used is to compute standard scores of the logarithm (respectively the reciprocal) of truth with respect to the logarithms (respectively the reciprocals) of the sample values. In the Viertelfest sample the logarithm of the truth is \Sexpr{format(standardScoreTruth(sD,truth),digits=3)} standard deviations away from the logarithms of estimates. In the Oldenburg sample the reciprocal of the truth is \Sexpr{format(standardScoreTruth(sO,truth_O),digits=3)} standard deviations away form the reciprocals of estimates. }. 

  The arithmetic mean, the geometric mean, the harmonic mean, as well as standard score, variance, collective error and mean of squared error, and thus the whole reasoning of the diversity predicition theorem relies on statistics based on moments. 
  Order statistics as the median are the typical alternative to them. 
  They are often used because they are more robust.
  In the limited amount of the three samples presented here the median has turned out as most robust aggregation function because it aggregates good collective estimates for two samples (Galton and Viertelfest). It has also show to deliver good collective estimates in \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence}.

 The wisdom-of-crowd indicator introduced by \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} relies on the median as the aggregation function for collective assessments. 
 For a sample of numbers $\{{x}_1,\dots,{x}_N\}$ we consider the sequence $\{\hat{x}_1,\dots,\hat{x}_N\}$ in ascending order. The \emph{wisdom-of-crowd indicator} is
 \begin{equation} 
  \textrm{WoC}(x,\textrm{truth}) = \frac{\max\{ i \,|\, \hat{x}_i \leq \textrm{truth} \leq \hat{x}_{N-i+1}\} }{\lceil N/2 \rceil}
 \end{equation}
 if the set in the nominator is not empty and zero if it is empty. 
 The brackets $\lceil \cdot \rceil$ are rounding to the next upper integer. 
 The idea of the indicator is to bracket the truth with two estimates from the sample which are equally close in position in the ordering and as close as possible to the median estimate. 
 The division by $\lceil N/2 \rceil$ is done because this is the index of the median value of an ordered odd sample of size $N$ and the first of the two median values of an even sample. Consequently, $\lceil N/2 \rceil$ is the maximal possible value of $\max\{ i \,|\, \hat{x}_i \leq \textrm{truth} \leq \hat{x}_{N-i+1}\}$.\footnote{In \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} the indicator is defined without the division by $\lceil N/2 \rceil$, because all samples were of equal size. It makes more sense to introduce it in the normalized form here to compare samples of different sizes.}
 The wisdom-of-crowd estimator equals one when the truth is bracketed between the two median estimates (for a sample of even size) or lies exactly at the median (for an odd sample). The wisdom-of-crowd indicator equals zero when the truth is outside of the range of estimates -- either below the minimal or above the maximal estimate. 
 The wisdom-of-crowd indicator can be described as the fraction of ordered estimates outside of the smallest centered truth-bracketing set of estimates. 
 
 The experiment of \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} showed that the wisdom-of-crowd indicator in the experimental groups in rounds 2 to 5 (after the round of initial estimates) is on average 0.65 in the control treatment with no information exchange. It was 0.23 less after feedback of aggregated information and 0.16 less after feedback of full information.\footnote{In \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} the values are 3.92, 1.39 and 0.98. In this text, the normalized values are reported which are a sixth of the original values, because group size was twelve.} 
 The effects were statistically significant ($p<0.05$ and $p<0.01$). 
 This effect is called the \emph{range reduction effect} because the range of estimates reduces through exchange of estimates. The reduction is such that the truth tends to lie in more peripheral regions and a larger fraction of the inner estimates is needed to bracket it. 
 
 In the samples of Galton and the Viertelfest the median has turned out to be an acceptable measure of aggregation, while it is far off in the Oldenburg sample (cf. Table \ref{tab:WoC}). The wisdom-of-crowd indicators are \Sexpr{format(WoCIndicator(G,truth_G),digits=3)} for Galton's sample, \Sexpr{format(WoCIndicator(sD,truth),digits=3)} for the Viertelfest, and \Sexpr{format(WoCIndicator(sO,truth_O),digits=3)} for the sample from Oldenburg. Hence, the Viertelfest sample shows the highest degree of crowd wisdom, because \Sexpr{format(100*(1-WoCIndicator(sD,truth)),digits=2)}\% of the inner estimates are enough to bracket the truth. In Galton's sample \Sexpr{format(100*(1-WoCIndicator(G,truth_G)),digits=3)}\% are needed. 

 The wisdom-of-crowd indicator helped to show that the range reduction effect through even mild social influence is strong enough to bring the truth to more periperal regions of samples  of estimates \parencite{Lorenz.Rauhut.ea2011HowSocialInfluence}. 
 Further on, its application for the larger samples of Galton and the Viertelfest shows that the wisdom-of-crowd indicator can quantify the degree of crowd wisdom quite well. 
 Intuitively, both samples show a high degree of crowd wisdom because they can be aggregated by the median to a surprisingly good collective estimate.
 The wisdom-of-crowd indicator captures this intuition because it computes similar values for both samples despite very different magnitudes of bias and dispersion and different shapes of the distribution. 

 
 \section{Experts in the crowd, optimal crowd size, and diversity sampling} \label{sec:wocnew}

 This section presents ideas how to derive optimal sizes of guesstimation crowds and how one should sample them based on the statistical conception of crowd wisdom. 
 The questions guiding the ideas are:
 \begin{compactenum}
 \item How many people are wiser than the crowd?
 \item Is there an optimal size of a crowd?
 \item How do we sample the crowd? 
 \end{compactenum}
 
 \paragraph{How many people are wiser than the crowd?}
 Let us start with the example of Galton's sample. 
 The mean estimate for the weight of the meat of the ox was \Sexpr{format(mean(G),digits=5,big.mark = ",")} pound, and the distance to the correct value of \Sexpr{format(truth_G,digits=4,big.mark = ",")} was as tiny as \Sexpr{format(abs(mean(G)-truth_G),digits=2)} pounds. 
 Nevertheless, there was one person estimating 1,197 and two estimating 1,199 pounds. 
 In some sense, these three individuals were ``wiser'' than the crowd!
 If we had asked one of these persons for a judgment, we would have had better information than by collecting and aggregating the judgments of all. 
 The problem is, that it is likely that these persons estimated well partly by chance and it is not easy or even impossible to detect them before we know the truth.\footnote{\textcite{Tetlock.Gardner2015SuperforcastingArtand} report research based on the ``Good Judgement Project'' which is about how to do exactly that. They present results and ten commandments how to make good predictions.}

 Let us call those individuals whose estimates is closer to the truth than a collective estimate the \emph{experts}. This definition of an expert does not rely on an assessment of deep competence in judgment but just declares good de facto estimators as de facto experts.\footnote{A de facto expert according to the definition in this section is by no means necessarily a ``superforcaster'' in the sense of \textcite{Tetlock.Gardner2015SuperforcastingArtand}. }
  Can we expect that experts exist for every guesstimation task? 
  What is the role of population bias?
  Do the answers to these questions inform us about optimal sample size for the aggregation of the wisdom of the crowd? 
  
  Let us approach the answers to these questions theoretically. 
  To that end, we consider individuals with their particular estimates to be random draws from a population. 
  A collective estimate of a crowd of $k$ individuals is the aggregate of $k$ independent draws from the same population. 
  An estimate is thus a random variable $X$ with a certain probability density function $f_X$. 
  A collective estimate of $k$ independent estimates $\bar{X}_k$ is another random variable which is a function of $k$ independent draws from random variables identically distributed as $X$. 
  Consequently, its probability density function $f_{\bar{X}_k}$ can thus be derived from $f_X$ and the aggregation function for the collective estimate.   
  The \emph{probability that a single estimate is closer to the truth as the collection of $k$ estimates} is thus
  \begin{equation} 
     P(|X-\textrm{truth}| < |\bar{X}_k-\textrm{truth}|). \label{eq:Pr1}
  \end{equation}
  
  In the following we consider $X$ to be normally distributed with mean $\mu$ and standard deviation $\sigma$ and the aggregation function for $k$ estimates to be the arithmetic mean. 
  For the sake of statistical simplicity we also assume that we draw the estimates of people with replacement. This assumption might seem unrealistic when we think of a competetion of a single individual against $k$ others from the same population. 
  Statistically, sampling with or without replacement would only make a big difference for very small populations. 
  Let us further assume without loss of generality that $\sigma = 1$ and $\textrm{truth}=0$.
  Then, $b = \mu$ is the (unsquared) bias in the population and $f_X(x) = \phi(x-b)$, with $\phi(x) = \frac{e^{-x^2/2}}{\sqrt{2\pi}}$ being the probability density function of the standard normal distribution with mean zero and standard deviation one. 
  For means of independent normally distributed random variables, it holds that $\bar{X}_k$ is also normally distributed with standard deviation $\frac{1}{\sqrt{k}}$. Hence, it holds $f_{\bar{X}_k}(x) = \frac{1}{\sqrt{k}}\phi(\frac{x-b}{\sqrt{k}})$.
  Now, the probability of receiving a better answer from one randomly selected person than with the average of $k$ persons (\ref{eq:Pr1}) is 
  \begin{align} P(|X| < |\bar{X}_k|) &= P(|X| - |\bar{X}_k| < 0) = \int_{-\infty}^0 f_{|X| - |\bar{X}_k|}(x) dx \nonumber \\ 
  & = \int_{-\infty}^0 f_{|X|}\ast f_{-|\bar{X}_k|}(x) dx\label{eq:Pr2}
  \end{align}
  where `$\ast$' is the convolution of the two functions\footnote{The convolution of functions $f$ and $g$ is defined for any $x$ as $f\ast g(x) = \int dy\, f(y)g(x-y)$}. The two functions in the convolution of (\ref{eq:Pr2}) are
  \begin{equation} 
    f_{|X|} = \begin{cases} \phi(x-b) + \phi(-x+b) & \text{if $x-b\geq 0$,} \\ 0 & \text{otherwise,} \end{cases}
  \end{equation}
and   
  \begin{equation} 
    f_{-|\bar{X}_k|} = \begin{cases} 0 & \text{if $x-b\geq 0$,} \\ \frac{1}{\sqrt{k}}\left(\phi(-\frac{x-b}{\sqrt{k}}) + \phi(\frac{x-b}{\sqrt{k}})\right) & \text{otherwise.} \end{cases}
  \end{equation}
\begin{figure}
<<WiserThanCrowdTheory, echo=F, fig.height=4>>=
b <- 0.5
k <- 11
fX <- function(x) dnorm(x,mean=b)
fXk <- function(x) dnorm(x,mean=b,sd=1/sqrt(k))
fabsX <- function(x) H(x)*fX(x)+H(x)*fX(-x)
fmabsXk <- function(x) H(-x)*fXk(x)+H(-x)*fXk(-x)
dx=0.01 # discretization
x=seq(-5,5,dx)
f=dx*convolution(fabsX(x),fmabsXk(x))[(length(x)-(length(x)-1)/2):(length(x)+(length(x)-1)/2)]
p1 <- ggplot(data.frame(x=c(-5,5)), mapping = aes(x)) + theme_bw() + 
  stat_function(fun=fX, aes(color="X")) + 
  stat_function(fun=fXk, aes(color="X10"), n=301) +
  annotate("pointrange", x=0, y = 1, ymin = 0, ymax = 1, colour = "darkviolet", size = 1) +
  annotate("text", label="Truth", x=0-0.3, y = 1, colour = "darkviolet", hjust=1) + ylab("probability density") +
  scale_colour_manual("Random Variables",values=c("red","blue"), breaks = c("X","X10"),
                      labels=c(paste0("X with b=",b),paste0("X",k," (mean ",k," draws)")))
p2 <- ggplot(data.frame(x=c(-5,5)), mapping = aes(x)) + theme_bw() +
  geom_area(data=data.frame(x=x[x<0],f=f[x<0]), aes(x=x,y=f), fill="black", alpha=0.2) + 
  stat_function(fun=fabsX, aes(color="X"), n=3001) + 
  stat_function(fun=fmabsXk, aes(color="X10"), n=3001) +
  geom_line(data=data.frame(x,f), aes(x=x,y=f,color="conv")) + ylab("probability density") +
  scale_colour_manual("Random Variables",
                      breaks = c("X","X10","conv"),
                      values=c("black","red","blue"), 
                      labels=c(paste0("|X|"),paste0("-|X",k,"|"), paste0("|X|-|X",k,"|"))) 
grid.newpage()
grid.draw(rbind(ggplotGrob(p1),ggplotGrob(p2), size="first"))
@
\caption{Example for the probability density functions of $X$ (standard normal distribution with bias $b=\Sexpr{b}$), $\bar{X}_k$ (average of $k=\Sexpr{k}$ versions of $X$, labelled X\Sexpr{k}), $|X|$, $-|\bar{X}_k|$, and $|X|-|\bar{X}_k|$. }\label{fig:WiserThanCrowdTheory}
\end{figure}

 Figure \ref{fig:WiserThanCrowdTheory} shows an example of these function for $b=\Sexpr{b}$. The gray area represents the probability that $|X|<|\bar{X}_k|$.  
 In the following, we compute the values of the integral over the convolution of Equation (\ref{eq:Pr2}) by numerical convolution and integration using a fine discretization $dx=\Sexpr{dx}$. 
 The precise computation is documented in \textcite{Lorenz2017Sourcefilesand}. 
  
  Now, we have a framework to analyze the impact of bias and crowd size on the probability to select an expert by chance. 
  This probability also gives us a measure for the number of experts which we should expect in a sample of $k$ individuals. 
  We just have to multiply the probability by $k$. 
  Let us first look at the case of no population bias $b=0$.
  \begin{figure}
<<WiserThanCrowdTheory2, echo=F, fig.height=3>>=
b <- 0
k <- 1:1000
if (file.exists("R/WiserThanCrowdTheory2.RData")) {
  load("R/WiserThanCrowdTheory2.RData")
} else {
  Probability <- rep(NA,length(k))
  for (ik in 1:length(k)) { Probability[ik] <- prob_1_better_k_normal(b, k[ik]) }
  save(Probability,file = "R/WiserThanCrowdTheory2.RData")
}
NumExperts <- Probability*k
p1 <- ggplot(data=data.frame(k,Probability), aes(x=k,y=Probability)) + theme_bw() + geom_line() + 
  ylab("Probability to Draw Expert") + xlab("Crowd Size k") +
  scale_x_continuous(expand=c(0,0), limits=c(0,1000)) + scale_y_continuous(expand=c(0,0), limits=c(0,0.5))
p2 <- ggplot(data=data.frame(k,NumExperts), aes(x=k,y=NumExperts)) + theme_bw() + geom_line() + 
  ylab("Expected Number of Experts") + xlab("Crowd Size k") +
  scale_x_continuous(expand=c(0,0), limits=c(0,1000)) + scale_y_continuous(expand=c(0,0), limits=c(0,21))
library(grid)
grid.newpage()
grid.draw(cbind(ggplotGrob(p1),ggplotGrob(p2), size="first"))
@
\caption{Probability of drawing an expert against a sample of $k$ individuals and number of experts in a sample of size $k$ when there is no bias ($b=0$). }\label{fig:WiserThanCrowdTheory2}
\end{figure}
  Figure \ref{fig:WiserThanCrowdTheory2} shows that the probability to draw an expert converges to zero with $k\to\infty$, while at the same the expected number of experts increases without saturation. 
  For a group of 1,000 agents the probability to find an expert is $\Sexpr{round(Probability[1000],digits=3)}$, and consequently the expected number of experts among these is $\Sexpr{round(NumExperts[1000])}$. 
 This picture changes with bias. 
 
\begin{figure}
<<WiserThanCrowdTheory3, echo=F, fig.height=3>>=
b <- 0.5
k <- 1:100
if (file.exists("R/WiserThanCrowdTheory3.RData")) {
  load("R/WiserThanCrowdTheory3.RData")
} else {
  Probability <- rep(NA,length(k))
  for (ik in 1:length(k)) { Probability[ik] <- prob_1_better_k_normal(b, k[ik]) }
  save(Probability,file = "R/WiserThanCrowdTheory3.RData")
}
NumExperts <- Probability*k
q1in <- ggplotGrob(
  ggplot(data=data.frame(k=k[10:60],P=Probability[10:60]), aes(x=k,y=P)) + theme_bw() + geom_line() +
  geom_point(data=data.frame(x=19,y=Probability[19]), aes(x=x,y=y)) + 
  labs(subtitle="Detail") +
  theme(axis.title.x=element_blank(), axis.title.y=element_blank()) +
  scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0), limits=c(0.336,0.341))
)
q1 <- ggplot(data=data.frame(k,Probability), aes(x=k,y=Probability)) + theme_bw() + geom_line() +
  ylab("Probability to Draw Expert") + xlab("Crowd Size k") +
  scale_x_continuous(expand=c(0,0), limits=c(0,100)) + scale_y_continuous(expand=c(0,0), limits=c(0,0.5)) +
  annotation_custom(grob=q1in, xmin=37,xmax=98,ymin=0,ymax=0.3)
q2 <- ggplot(data=data.frame(k,NumExperts), aes(x=k,y=NumExperts)) + theme_bw() + geom_line() +
  ylab("Expected Number of Experts") + xlab("Crowd Size k") +
  scale_x_continuous(expand=c(0,0), limits=c(0,100)) + scale_y_continuous(expand=c(0,0), limits=c(0,35))
library(grid)
grid.newpage()
grid.draw(cbind(ggplotGrob(q1),ggplotGrob(q2), size="first"))
@
\caption{Probability of drawing an expert against a sample of $k$ individuals and number of experts in a sample of size $k$ when there is a bias of $b = \Sexpr{b}$.}\label{fig:WiserThanCrowdTheory3}
\end{figure}
 
 \paragraph{Optimal crowd size in biased populations}
 Figure \ref{fig:WiserThanCrowdTheory3} shows an example for the bias $b=\Sexpr{b}$. 
 In the limit of $k\to\infty$ the probability to draw an expert now saturates at $\Sexpr{round(prob_1_better_k_normal(b, Inf),digits=3)}$. 
 Consequently, the number of experts increases linearly for large $k$. 
 Interestingly, the probability to draw an expert does not decline monotonically. 
 There is an interior minimum at $k=\Sexpr{which(Probability==min(Probability))}$ for which the probability is lowest (\Sexpr{round(min(Probability),digits=3)}).
 Thus, for a bias of \Sexpr{b} it is in some sense optimal to ask \Sexpr{which(Probability==min(Probability))} individuals and take their avergae estimate.
 The explanation for this surprising intermediate optimum under some bias is the following. 
 For a lower crowd size the group is just not large enough to optimally outperform a randomly drawn individual. 
 Larger groups are better for the same reason as in the unbiased situation. 
 With bias another phenomenon occurs. 
 For large groups the dispersion of the collective estimate vanishes, such that we end up with a collective estimate very close to the bias with high certainty.  
 In contrast, for a draw of a single individual there is a chance to receive the almost correct answer. 
 The optimal crowd size is achieved when the variance decreasing effect is balanced optimally against the effect that larger groups will deliver a wrong answer for sure. 
 
\begin{figure}
<<WiserThanCrowdTheory4, echo=F, fig.height=3>>=
b <- seq(0.1,2.5,by=0.01)
k <- Inf
limProb <- rep(NA,length(b))
for (ib in 1:length(b)) { limProb[ib] <- prob_1_better_k_normal(b[ib], k) }
if (file.exists("R/WiserThanCrowdTheory4.RData")) {
  load("R/WiserThanCrowdTheory4.RData")
} else {
  opt_k <- rep(1,length(b))
  min_P <- rep(1,length(b))
  for (ib in rev(1:length(b))) {
    tmp <- optimal_k_normal(b[ib], initial_k = opt_k[min(ib+1,length(b))])
    opt_k[ib] <- tmp$k
    min_P[ib] <- tmp$P
  }
  save(opt_k,min_P,file = "R/WiserThanCrowdTheory4.RData")
}
D <- data.frame(b,limProb,opt_k,min_P)
D1 <- gather(D,key=key,value=Prob,limProb,min_P)
r1 <- ggplot(data=D1, aes(x=b,y=Prob,color=key)) + theme_bw() + geom_line() +
  scale_y_continuous(expand=c(0,0), limits=c(0,0.51)) + scale_x_continuous(expand=c(0,0),limits=c(0,max(b))) +     scale_colour_manual("Probability to draw expert", breaks = c("limProb","min_P"),
                      values=c("blue","black"), labels=c("For k=Inf ","For optimal k")) +
  theme(legend.position = c(0.6,.2), legend.direction="vertical") +
  ylab("Probability") + xlab("bias b")
r2in <- ggplotGrob(ggplot(filter(D,b<1.5,b>0.5), aes(x=b,y=opt_k), color="black") + theme_bw() + geom_line() +
                     scale_y_continuous(expand=c(0,0),limits=c(1,20)) + scale_x_continuous(expand=c(0,0)) + 
                     labs(subtitle="Detail") + ylab("") + xlab("")
)
r2 <- ggplot(D, aes(x=b,y=opt_k), color="black") + theme_bw() + geom_line() +
  scale_y_continuous(expand=c(0,0)) + scale_x_continuous(expand=c(0,0),limits=c(0,max(b))) +
  annotation_custom(grob=r2in, xmin=0.5,xmax=2.4,ymin=300,ymax=1100) +
  ylab("Optimal Crowd Size k") + xlab("bias b")
grid.newpage()
grid.draw(cbind(ggplotGrob(r1),ggplotGrob(r2), size="first"))
@
\caption{The impact of the bias $b$ on the probability to draw an expert (first panel) and the optimal crowd size for which this probability is lowest. The blue line in the first panel is the probability for large groups ($k=\infty$), the black line the probability for the optimal $k$ as plotted in the second panel. The deviation of the blue and the black lines represents the risk of having worse collective estimates by collecting too many estimates. This region ($0.5<b<1.5$) is focussed in the inset in the second panel. }\label{fig:WiserThanCrowdTheory4}
\end{figure}

 In Figure \ref{fig:WiserThanCrowdTheory4} we further analyse the impact of the bias on the probability to draw an expert and the optimal crowd size. 
 From this figure and the results before we derive the following insights:
 \begin{compactitem}
 \item In a large crowd with a large bias, half of the people are ``experts'' because they outperform the mean estimate\footnote{Note, that this result generalizes in its strict form only for symmetric distributions.}. Precisely, with increasing bias the probability to draw an expert from a large crowd increases and saturates at 0.5. For a bias of 1.5 standard deviations it is already $\Sexpr{round(D[round(D$b,digits=2)==1.5,"limProb"],digits=3)}$.  
 \item In a large crowd with small bias, experts are rare, and the optimal size of a group is very large. For $b<\Sexpr{max(D[D$opt_k>=100,"b"])}$ the optimal crowd size is larger than 100, for $b<\Sexpr{max(D[D$opt_k>=500,"b"])}$ it is larger than 500 and for $b<\Sexpr{max(D[D$opt_k>=1000,"b"])}$ larger than 1,000. 
 \item In a crowd with no bias, the share of experts goes to zero, although their number increases slowly without saturation. 
 \item When the bias is between 0.5 and 1.5 times the standard deviation, the optimal crowd size is between $\Sexpr{D[round(D$b,digits=2)==0.5,"opt_k"]}$ and $\Sexpr{D[round(D$b,digits=2)==1.5,"opt_k"]}$. In this range of bias there is also a notable difference of the probability to draw expert which is better than a large crowd ($k\to\infty$) and a crowd of optimal size. Thus, in this range of bias there is a risk of drawing to many estimates and receive worse results with high likelihood. 
 \end{compactitem}

 The theory and results so far were presented and computed under the assumptions that estimates are normally distributed and that the arithmetic mean is used for the aggregation of collective estimates. 
 The results for other distributions of estimates and other appropriate measures of aggregation are probabily essentially similar regarding the general characteristics. 
 Firstly, the mean of several draws from the same distribution, even when this distribution is not normal, converges to be normally distributed due to the central limit theoerm. The decrease of standard deviation with $\sqrt{k}$ holds for all distributions with finite variance. 
 Further on, the convergence to the normal distribution with standard deviation $\sqrt{k}$ also holds for the median as the measure of aggregation \parencite{Bahadur1966NoteQuantilesin}. 

 Let us finally look at the three empirical samples Galton, Viertelfest, and Oldenburg. 
 As we already discussed, the median was a good aggregation function for both the Galton and the Viertelfest sample. 
 For a sample of $n$ numbers it is relatively straightforward, to compute a weight vector of the same size where each weight represents the probabilty that this value turns out to be the median of $k$ independent and identically distributed draws from the sample with replacement. The computation is documented in \textcite{Lorenz2017Sourcefilesand}.
 Figure \ref{fig:WiserThanCrowdData} shows the histograms of the draws from the Galton and the Viertelfest
samples together with histograms of the median of twenty (respectively) two hundred independent draws from these samples with replacement. 
 These examples show how it becomes almost certain to receive slightly wrong collective estimates for large crowds. 
\begin{figure}
<<WiserThanCrowd,echo=FALSE, cache=T>>=
K <- c(1,20,200)
Knames=c("Wk1"="1 draw", "Wk20"="Median 20 iid draws", "Wk200"="Median 200 iid draws")
E <- data_frame(G)
for (k in K) {E[,paste0("Wk",k)] = weights_median_n_k(nrow(E),k)}
E <- gather(E,key = key, value=weight, -G)
g1 <- ggplot(filter(E,G>1080,G<1320), aes(G, weight=weight, alpha=0.8)) + 
  geom_histogram(binwidth = 5, position='identity', fill="blue") + 
  facet_grid(key ~ ., scales="free_y") +
  annotate("pointrange",y=0,x=truth_G, ymin=0,ymax=0) +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none", strip.text.y = element_blank()) +
  ggtitle("Galton") + xlab("Estimates (axis clipped)") + ylab("probability")
D <- data_frame(sD=sort(sD))
for (k in K) {D[,paste0("Wk",k)] = weights_median_n_k(nrow(D),k)}
D <- gather(D,key = key, value=weight, -sD)
g2 <- ggplot(filter(D, sD>0, sD<40000), aes(sD, weight=weight, alpha=0.8)) + 
  geom_histogram(binwidth = 1000, position='identity', fill="red") + 
  facet_grid(key ~ ., scales="free_y",labeller = as_labeller(Knames)) +
  annotate("pointrange",y=0,x=truth, ymin=0,ymax=0) + ylab("") + ggtitle("Viertelfest") + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") + xlab("Estimates (axis clipped)")
library(grid)
grid.newpage()
grid.draw(cbind(ggplotGrob(g1),ggplotGrob(g2), size="first"))
@
\caption{Histograms for the Galton and the Viertelfest samples (1 draw) togehter with histrograms for the median of 20 and 200 independ and identically distributed draws from the same sample with replacement. The black dot is the true value.  } \label{fig:WiserThanCrowdData}
\end{figure}
 
 Finally, Table \ref{tab:WoC2} summarizes the results about the number of experts and the optimal group size for the three empirical samples (cf.~Table \ref{tab:WoC}). The results show that the optimal group size is largest for the Viertelfest sample. This presents this sample as the one with the highest degree of crowd wisdom with respect to median aggregation, which is in line with the wisdom-of-crowd indicator being lowest for this sample (cf.~Section \ref{sec:wocindicator}). 

 \begin{table}
\small
\begin{tabular}{>{\raggedleft\arraybackslash}m{0.38\textwidth}>{\raggedleft\arraybackslash}m{0.16\textwidth}>{\raggedleft\arraybackslash}m{0.16\textwidth}>{\raggedleft\arraybackslash}m{0.16\textwidth}} \toprule 
 & Galton & Viertelfest & Oldenburg \\ \midrule\midrule
Number of estimates $N$ & \Sexpr{length(G)} & \Sexpr{format(length(sD),big.mark=",")} & \Sexpr{length(sO)} \\ \addlinespace[0.5ex]
True value & \Sexpr{format(truth_G,big.mark=",")} & \Sexpr{format(truth,big.mark=",")} & \Sexpr{format(truth_O,big.mark=",")} \\ \addlinespace[0.5ex]
\# estimates closer to truth then best mean (\%) [best mean] & \Sexpr{format(sum(abs(powermean(G,1)-truth_G)>abs(G-truth_G)),digits=3)} (\Sexpr{format(100*sum(abs(powermean(G,1)-truth_G)>abs(G-truth_G))/length(G),digits=2)}\%) [Arithmetic] & \Sexpr{format(sum(abs(powermean(sD,0)-truth)>abs(sD-truth)),digits=3)} (\Sexpr{format(100*sum(abs(powermean(sD,0)-truth)>abs(sD-truth))/length(sD),digits=2)}\%) [Geometric] & \Sexpr{format(sum(abs(powermean(sO,-1)-truth_O)>abs(sO-truth_O)),digits=3)} (\Sexpr{format(100*sum(abs(powermean(sO,-1)-truth_O)>abs(sO-truth_O))/length(sO),digits=2)}\%) [Harmonic] \\ \addlinespace[0.5ex]
\# estimates closer to truth then median (\%)& \Sexpr{format(sum(abs(median(G)-truth_G)>abs(G-truth_G)),digits=3)} (\Sexpr{format(100*sum(abs(median(G)-truth_G)>abs(G-truth_G))/length(G),digits=2)}\%) & \Sexpr{format(sum(abs(median(sD)-truth)>abs(sD-truth)),digits=3)} (\Sexpr{format(100*sum(abs(median(sD)-truth)>abs(sD-truth))/length(sD),digits=2)}\%)  & \Sexpr{format(sum(abs(median(sO)-truth_O)>abs(sO-truth_O)),digits=3)} (\Sexpr{format(100*sum(abs(median(sO)-truth_O)>abs(sO-truth_O))/length(sO),digits=3)}\%)  \\ \addlinespace[0.5ex]
optimal crowd size for median of iid draws against one draw (prob. that one draw is better) & \Sexpr{format(optimal_k_median_S(G,truth_G,initial_k=550)$k,big.mark=",")} (\Sexpr{format(100*optimal_k_median_S(G,truth_G,initial_k=550)$P,digits=3)}\%) & \Sexpr{format(optimal_k_median_S(sD,truth,initial_k=2870)$k,big.mark=",")} (\Sexpr{format(100*optimal_k_median_S(sD,truth,initial_k=2870)$P,digits=3)}\%)  & \Sexpr{format(optimal_k_median_S(sO,truth_O)$k,big.mark=",")} (\Sexpr{format(100*optimal_k_median_S(sO,truth_O)$P,digits=3)}\%)  \\ \addlinespace[0.5ex]
 \bottomrule
\end{tabular}
\caption{The three example datasets on the wisdom of the crowd. Number of people who are better then the crowd with respect to the best mean (cf.~Table \ref{tab:WoC}) and the median. The optimal crowd size for independent and identically distributed draws (iid) from the sample (with replacement) such that the probability to draw a better single estimates is lowest. } \label{tab:WoC2}
\end{table}



\paragraph{Diversity sampling}
 In the previous paragraph we saw that whenever there is a small bias in the population, there is an optimal crowd size to compete optimally with single individuals. 
 This paragraph deals with the process of sampling the crowd. 
 Is there a way to reduce the collective error by rules which newly sampled estimates to accept, based on characteristics of the already sampled estimates?
 In particular, we will analyze the idea to sample for \emph{diversity}.
 
 The idea of diversity sampling is to reject new draws which are too close to the current average of the sample. The full code of the method is available in the source files of this work \parencite{Lorenz2017Sourcefilesand}. 
 Let us think of iterative sampling of $x_1, x_2, \dots$ as independent draws from the same sample one after the other.
 Let us call $\bar{x}_i$ the mean of all sampled estimates up to estimate $i$. 
 Let us further call $\text{SEM}(x_1,\dots,x_i) = \frac{s_i}{\sqrt{i}}$ be the \emph{standard error of the mean} of the first $i$ estimates, where $s_i$ is the \emph{sample standard deviation}\footnote{$s_i=\sqrt{\frac{1}{i}\sum_{j=1}^i(x_j-\bar{x}_i)^2}$}. 
 The sampling procedure is now the following:
 \begin{compactenum}
 \item Sample the first two estimates $x_1$ and $x_2$ as independent random draws with replacement from the sample. 
 \item For $i>2$, sample estimate $x_i$ such that $|x_i - \bar{x}_{i-1}| > \text{SEM}(x_1,\dots,x_{i-1})$.
 \end{compactenum}
 Technically, sampling under certain restrictions can be done in practice by normal random sampling and discarding any sample value which does not fulfill the desired conditions. 
 Note, that the standard error of the mean decreases with the square root of $i$. Therefore, diversity sampling approaches standard random sampling with increasing $i$. 

\begin{figure}
<<DiversitySampling, echo=FALSE>>=
source("R/diversitysampling.r")
if (file.exists("R/DiversitySamplingData.RData")) {
  load("R/DiversitySamplingData.RData")
} else {
  x <- "rnorm"
  n <- 5000 # sample size for each test
  tr <- 0
  test <- rbind(arrange_test(make_n_Samples(x, crowdsize=5, n, aggregation.fun = "mean", replace=TRUE),tr),
                arrange_test(make_n_Samples(x, crowdsize=10, n, aggregation.fun = "mean", replace=TRUE),tr),
                arrange_test(make_n_Samples(x, crowdsize=20, n, aggregation.fun = "mean", replace=TRUE),tr),
                arrange_test(make_n_Samples(x, crowdsize=50, n, aggregation.fun = "mean", replace=TRUE),tr),
                arrange_test(make_n_Samples(x, crowdsize=5, n, aggregation.fun = "median", replace=TRUE),tr),
                arrange_test(make_n_Samples(x, crowdsize=10, n, aggregation.fun = "median", replace=TRUE),tr),
                arrange_test(make_n_Samples(x, crowdsize=20, n, aggregation.fun = "median", replace=TRUE),tr),
                arrange_test(make_n_Samples(x, crowdsize=50, n, aggregation.fun = "median", replace=TRUE),tr))
  save(x,n,tr,test,file = "R/DiversitySamplingData.RData")
}
g1 <- ggplot(test,aes(y=collErr, x=key, color=key)) + geom_boxplot() +
  facet_grid(aggregation.fun ~ crowdsize, scales = "free_y") +
  labs(x="Sampling Method", y="(unsquared) Collective Error", 
       subtitle=paste0("Boxplots (",n," crowds per sample)")) + 
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))
stest <- test %>% group_by(aggregation.fun,crowdsize,key) %>% 
  summarise(avCollErr = mean(collErr), stdErr=SEM(collErr))
g2 <- ggplot(stest,aes(x=key, y=avCollErr, color=key)) + 
  geom_pointrange(aes(ymin = avCollErr-2.81*stdErr, ymax =  avCollErr+2.81*stdErr)) +
  facet_grid(aggregation.fun ~ crowdsize, scales = "free") + 
  labs(x="Sampling Method", y="", subtitle="Mean (with 99.5% confidence intervals)") +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) 
grid.newpage()
grid.draw(cbind(ggplotGrob(g1),ggplotGrob(g2),size="first"))
@
\caption{\textbf{Diversity sampling.} Boxplots and mean of (unsquared) collective errors of collective estimates of simulated samples of small crowds ($k=5,10,20,50$) each drawn from the standard normal distribution using random and diversity sampling. Crowds' collective estimates aggregated by the mean and the median. For each point \Sexpr{format(n,big.mark=",")} crowds have been sampled. 99.5\% confidence intervals are shown. Sometimes they lie within the area of the points. } \label{fig:DiversitySampling}
\end{figure}


 Figure \ref{fig:DiversitySampling} shows the results for several tests of diversity and usual random sampling. 
 Crowd sizes of $k=5,10,20$ and 50 have been used. For each crowd size and each strategy \Sexpr{n} crowd have been drawn from standard normal distributions. 
 Crowds' sampled values were aggregated by the mean and the median and the collective error was the distance to the correct value (zero in this case). 
 Errors were not squared.
 Figure \ref{fig:DiversitySampling} shows boxplots of the collective error and point estimates of the average collective error with very conservative confidence intervals 
 The results are mixed.
 Means of crowds lie on average significantly closer to the truth when diversity sampling is used instead of pure random sampling. 
 Probably, diversity sampling makes the first draws better centered around the center. 
 Consider for example that draws number one and two are larger than the mean, then diversity sampling makes it much more likely, that the third draw (which is the first one using diversity sampling restrictions) is below the mean. 
 Contrastingly, diversity sampling leads to larger collective error of the median of crowds which were sampled for diversity. As diversity sampling prevents new draws which are very central in the current sample an improvement of the median estimate does not occur as fast as with random sampling. 
 
 Nevertheless, it is surprising that improvements of random sampling are possible, because no knowledge about the distribution is used except for the already sampled values. 


\section{Crowd wisdom and collective decision}

 The collective estimates we analyzed so far were not really collective decisions. 
 They were only collective decisions from our external perspective, because we aggregated them to one. 
 Neither did the people contribute to the aggregation, nor did they knew about our intention. 
 Contrastingly, a lot of our statistical theory on crowd wisdom relies on the assumption that estimates are independent and thus neither coordinated nor influenced by each other. 
 We already looked at an experiment on the impact of social influence in the very mild form of information about the estimates of others \parencite{Lorenz.Rauhut.ea2011HowSocialInfluence}. 
 In this experiment the correctness of individual estimates was monetarily incentivized, but there was no need and no other incentive to agree or converge towards a collective decision besides the common knowledge that there was a universal truth valid for all. 
 
 \textcite{Lorenz.Rauhut.ea2015MajoritarianDemocracyUndermines} study the impact of collective decision rules on the truth-finding capacity of crowds which could communicate about common guesstimation tasks by means of a laboratory experiment where subject sat in cubicles in front of computer terminals without verbal or face-to-face communication with other subjects. 
 The underlying fundamental research question is if democratic principles are a mechanism to elicit crowd wisdom.
 Guesstimation tasks provide an objectively verifiable framework for crowd wisdom. 
 This maps to the insitution of advisory or expert committees which task is to predict future developments of for example economic growth, tax revenue, societal, economic, or environmental risks and so on. 
 These boards and committees do not always strictly decide based on democratic principles, but they are often expected to provide a common assessment and members are often picked based on different expertise and theoretical background. 
 Further on, they should predict facts which are to become verfiable in the future. 
 Thus, these committees face a demand to deliver a collective decision and this demand is underpinned epistemically by the nature of the task. 
 Ultimately, with respect to facts there is nothing but uncertainty to disagree about.
 This discussion about the role of crowd wisdom for democracy, should not blur the fact that many democratic decisions, like elections and referendums, do not provide this immediate objectivity with respect to correctness, but are for example solutions of conflicts of interest or value judgments. 
 In that sense, democractic decisions are to a large extent an instance of crowd wisdom by postulation.

 \textcite{Lorenz.Rauhut.ea2015MajoritarianDemocracyUndermines} analyze the impact and interplay of three democratic principles: majoritarianism, unanimity (or consensus), and deliberation. 
 ``Deliberation'' is meant here in a weak sense, meaning communication to achieve a solution for the common good.
 This is realized by incentives for the correctness of collective estimates. 
 This notion of deliberation is not intended to meet all the criteria for deliberation according to \textcite{Steenbergen.Bachtiger.ea2003MeasuringPoliticalDeliberation} and \textcite{Baechtiger.Niemeyer.ea2010DisentanglingDiversityin}. 
 
 As in the experiment of \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence}, subjects had to estimate facts on which they could exchange estimates. In each of nine rounds, subjects were put together anonymously at random in groups of three or nine subjects. 
 Before each round subjects were asked for an initial estimate which was never communicated to other subjects. 
 These initial estimates serve as independent estimates from subjects in the analysis. 
 For the deliberation phase, two different modes of deliberation were used: 
 \begin{compactenum}[(a)]
\item \emph{Numercial communication:} As in \textcite{Lorenz.Paetzel.ea2016JustDontCall} subjects had to communicate numerical proposals over ten rounds before they entered their individual final decisions. There was no way to exchange verbal statements or anything else then numbers. 
\item \emph{Chats:} Subjects had an ordinary chat window, in which all members could post textual messages which immediately appeared in the chat window of all subjects in the group. This chat windows closed after three minutes or when all subjects clicked a button that they were ready. 
\end{compactenum}
 After the deliberation subjects had to enter their final decision privately.  
 The decision of all subjects from a group were then aggregated to a final decision based on three different agreement rules for valid collective decisions:
 \begin{compactenum}[(i)]
\item \emph{Individual:} There was no need of a collective agreement. All subjects were paid independently based on the correctness of their indidivual decision. 
\item \emph{Majority:} If there was a proposal which was entered by more than half of the subjects, then this was made the collective decision. All subjects were then paid based on the correctness of the majority decision also when they disagreed. When subjects failed to agree, there was no payment for anyone. 
\item \emph{Unanimity:} Only if the same proposal was entered by all subjects, this was accepted as the collective decision. When subjects failed to agree, there was no payment for anyone, otherwise they were paid based on the correctness of their collective estimate. 
\end{compactenum}
 Further on, subjects were asked about their subjective certainty about their initial estimate as well as about their final decisions. 

 Similar to \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} there was no clear trend for the movement of the median of initial estimates to the median of the final decisions. 
 Some groups improved through deliberation, some worsened. 
 Some questions seem to make groups more prone to improvement of the median estimate through communication and other questions more to worsening.
 Sometimes, deliberation reduces bias, sometimes the opposite. 
 One trend instead, is clear in this dataset: 
 Groups who had to come up with a majority decision more often came to decisions worse than the median of their initial independent estimates. It was the other way round, when groups had to find a unanimous decision as well as in groups with individual payments without the need of agreement. In the latter groups the median of decisions was used as the extracted collective estimate.
 
 The inferiority of majority rule is surprising especially because the median voter theorem as introduced by \textcite{Downs1957BookEconomicTheoryof} (building on the works of \textcite{Hotelling1929StabilityinCompetition} and \textcite{Black1948RationaleofGroup}) says majority rule should bring about the median as the stable decision. 
 Together with Galton's plea for the median as the more robust measure of aggregation \parencite{Galton1907Onevoteone} it looks like majority rule should bring about the wisdom of the crowd. 
 The results of \textcite{Lorenz.Rauhut.ea2015MajoritarianDemocracyUndermines} show that this does not realize when deliberation is added to the process. 
 Besides the empirical fact, no clear mechanism causing the difference could be identified so far. 
 On the one hand, a forced collective decision could decrease independence of judgers even more than just information exchange with negative effects on group judging performance, but this should then also hold for groups with unanimity rule. 
 On the other hand, one might think that unanimity rule makes every subject's estimate relevant, while some can be ignored under majority rule, but exactly that possiblity is also sometimes seen as a benefit of majority rule.  Stupid or even malicious outliers can safely be ignored under majority rule. 
 One could speculate that majority rule has two effects which make it likely that the group performs worse than under unanity and individual rule. 
 First, majority rule makes people think quickly about where the majority decision must lie, this distracts from fruitful thoughts and discussion of the topic which could reduce bias. 
 Although this effect should also occur under unanimity rule, it is not that strong, because, for negotiation of a unanimous decision, one must first hear every single subject. 
 Second, majority rule might create a view that the own opinion is not important because it can be overruled anyway. 
 This could effect individual motivation upfront. 

 Another result of \textcite{Lorenz.Rauhut.ea2015MajoritarianDemocracyUndermines} is that the increase of self-reported certainty is highest under majority rule. Thus, groups under majority rule deliver on average the worst  collective decisions but with most average confidence. 

\newpage


\section{Discussion and conclusions on crowd wisdom}

 In this chapter we studied crowd wisdom in a simple but well-defined framework: numerical guesstimation tasks. 
 The experimental setup on numerical guesstimation and social influence of \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} is mentioned in some recent works with similar experimental setups. 

 \textcite{Guercay.Mellers.ea2015PowerSocialInfluence} have a similar setup which shows the power of social influence on estimation accuracy, which is somehow contradicting the undermining effect of social influence on crowd wisdom.
 A closer inspection reveals that the different conclusion is mostly due to different notions of crowd wisdom. 
 In both data sets individual trueness improves through social influence, while the aggregated group trueness\footnote{We use ``trueness'' here instead of ``accuracy'' based on the ISO 5725 definition, where accuracy is trueness and precision (meaning low variation). See also Section \ref{sec:quantwoc}.} improves only very little with large variation over tasks. Worsening is also observed on the collective level. 
 \textcite{Guercay.Mellers.ea2015PowerSocialInfluence} emphasize that there is at least some potential of reduction of collective bias through social influence. \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} instead emphasize that improvements of group trueness are small on average with large variation putting that into perspective with a large reduction of individual variance, creating potentially a collective tunnel vision. 

 \textcite{Hertz.RomandMonnier.ea2016Socialinfluenceprotects} attribute potential benefits of social influence in the reduction of equality bias, which they show through a manipulated perception task where one subject had a much harder task than the other. 
 Subjects which can not communicate would probably see both players as equally competent. 
 It remains to find out if this mechanism is dominant when groups improve through communication in numerical guesstimation and when in realistic tasks. 

 \textcite{Joensson.Hahn.ea2015kindgroupyou} show some evidence that small world network structures instead of fully connected networks are able to sustain some diversity for the benefit of crowd wisdom.  

 \textcite{Granovskiy.Gold.ea2015IntegrationSocialInformation} have results and interpretations similar to \textcite{Lorenz.Rauhut.ea2011HowSocialInfluence} and provide some insights on how the structure of the guesstimation space (0-100\% in this case) matters. 

 In my perception, most disagreement in interpretations of results of the positive or negative effect of social influence on crowd wisdom is due to the definition of crowd wisdom on which this chapter provides a thorough discussion. 
 
 These are some summarized conclusions about crowd wisdom:
\begin{compactitem}
 \item Relying on crowd wisdom assumes that there is something in the aggregation which goes beyond individual wisdom. 
 \item Often it is a good idea to use collective estimates but it is also important to communicate group diversity. 
 \item When communication cannot be precluded (which is the case in an open society) be aware of the possiblity of a collective tunnel vision. 
 \item Whenever possible it is a good idea to extract independent initial estimates of estimators. 
 \item In any team which is to estimate facts, avoid majoritarian thinking.
 \item Do not take increasing average confidence in a discussing group as a sign for your increased confidence in the collective estimate.
 \item For a prediction team where bias is repeatedly found to be larger then 0.5 standard deviations, it makes sense to think about a maximal crowd size. 
\end{compactitem}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%% CHAPTER 5 %%%%%%%%%%%%%%%%%
% \chapter{Synthesis}
% 
%  As conclusion and outlook let us discuss a question where opinion dynamics, societal growth, and the wisdom of the crowd are intertwined: Can democratic decisions bring about wise decisions about redistribution?
%  
%  We focus on the redistributive mechanism of \textcite{Meltzer.Richard1981RationalTheoryof}, proportional taxation and redistribution in equal lump-sums. To knowledge, this mechanism is not implemented anywhere in its pure monetary form, but nevertheless it is very relevant
%  
%  It is one of the simplest mechanism to implement a condition-free basic income which is among the most discussed next steps for a fundamental reform of the welfare states.
% 

% Models always simplify the real-world therefore in some sense every model is wrong in capturing reality. The quality of a model relies on the selection of relevant aspects in relevant relations such that the phenomena of interest are captured most appropriate by few variables and few parameters. 

\printbibliography

\end{document}